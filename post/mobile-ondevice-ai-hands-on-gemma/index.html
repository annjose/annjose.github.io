<!DOCTYPE html>
<html lang="en">

<head>
  <meta http-equiv="content-type" content="text/html; charset=utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <meta name="description"
    content="Practical guide to run mobile-optimized models like Gemma across mobile platforms">
  <meta name="generator" content="Hugo 0.121.1">

  <title>Hands-On: Mobile AI with Gemma - iOS, Android &middot; Reflections</title>

  
  
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/pure/1.0.0/pure-min.css">

  <!--[if lte IE 8]>
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/pure/1.0.0/grids-responsive-old-ie-min.css">
  <![endif]-->
  <!--[if gt IE 8]><!-->
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/pure/1.0.0/grids-responsive-min.css">
  <!--<![endif]-->

  <!--[if lte IE 8]>
  <link rel="stylesheet" href="https://annjose.com/css/side-menu-old-ie.css">
  <![endif]-->
  <!--[if gt IE 8]><!-->
  <link rel="stylesheet" href="https://annjose.com/css/side-menu.css">
  <!--<![endif]-->

  <link rel="stylesheet" href="https://annjose.com/css/blackburn.css">

  
  
  <script src="https://kit.fontawesome.com/3e898cf239.js" crossorigin="anonymous"></script>
  
  
  <link rel="preconnect" href="https://fonts.googleapis.com">
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>

  
  

  
  
  

  
  

  

  <link rel="shortcut icon" href="https://annjose.com/img/favicon.ico" type="image/x-icon" />

  
  

</head>

<body>
<div id="layout">

  
<a href="#menu" id="menuLink" class="menu-link">
  
  <span></span>
</a>
<div id="menu">

  
  <a class="pure-menu-heading brand" href="https://annjose.com/">Reflections</a>


  <div class="pure-menu">
    <ul class="pure-menu-list">
      
      
        <li class="pure-menu-item">
          <a class="pure-menu-link" href="https://annjose.com/"><i class='fas fa-home fa-fw'></i>Home</a>
      
        </li>
      
      
        <li class="pure-menu-item">
          <a class="pure-menu-link" href="https://annjose.com/post/"><i class='fas fa-pen-to-square fa-fw'></i>My Blog</a>
      
        </li>
      
      
        <li class="pure-menu-item">
          <a class="pure-menu-link" href="https://annjose.com/tags/LLM"><i class='fas fa-hexagon-nodes fa-fw'></i>AI &amp; LLMs</a>
      
        </li>
      
      
        <li class="pure-menu-item">
          <a class="pure-menu-link" href="https://annjose.com/tags/tech-explorations"><i class='fas fa-magnifying-glass fa-fw'></i>Tech Explorations</a>
      
        </li>
      
      
        <li class="pure-menu-item">
          <a class="pure-menu-link" href="https://annjose.com/tags/personal-growth"><i class='fas fa-arrow-up-right-dots fa-fw'></i>Personal Growth</a>
      
        </li>
      
      
        <li class="pure-menu-item">
          <a class="pure-menu-link" href="https://annjose.com/tags/web-development"><i class='fas fa-globe fa-fw'></i>Web Development</a>
      
        </li>
      
      
        <li class="pure-menu-item">
          <a class="pure-menu-link" href="https://annjose.com/tags/mobile-tech"><i class='fas fa-mobile-screen fa-fw'></i>Mobile Tech</a>
      
        </li>
      
      
        <li class="pure-menu-item">
          <a class="pure-menu-link" href="https://annjose.com/about/"><i class='fas fa-user fa-fw'></i>About Me</a>
      
        </li>
      
    </ul>
  </div>

  <div class="pure-menu social">
  <ul class="pure-menu-list">

    

    

    
    <li class="pure-menu-item">
      <a class="pure-menu-link" href="https://bsky.app/profile/annjose.com" target="_blank"><i class="fa-brands fa-square-bluesky fa-fw"></i>Bluesky</a>
    </li>
    

    

    

    

    

    

    

    

    

    

    

    

    

    

    

    

    

    
    <li class="pure-menu-item">
      <a class="pure-menu-link" href="https://linkedin.com/in/annjose" target="_blank"><i class="fa fa-linkedin-square fa-fw"></i>LinkedIn</a>
    </li>
    

    

    

    

    
    <li class="pure-menu-item">
      <a class="pure-menu-link" href="https://news.ycombinator.com/user?id=annjose" target="_blank"><i class="fa fa-hacker-news fa-fw"></i>Hacker News</a>
    </li>
    

    

    
    <li class="pure-menu-item">
      <a class="pure-menu-link" href="https://github.com/annjose" target="_blank"><i class="fa fa-github-square fa-fw"></i>GitHub</a>
    </li>
    

    

    

    

    

    

    

    

    

    

    

    

    

  </ul>
</div>


  <div>
  <div class="small-print">
    <small>&copy; 2025. All rights reserved.</small>
  </div>
  <div class="small-print">
    <small>Built with<a href="https://gohugo.io/" target="_blank">Hugo</a></small>
  </div>
</div>

</div>


  <div id="main">


<div class="header">
  <h1>Hands-On: Mobile AI with Gemma - iOS, Android</h1>
  <h2>Practical guide to run mobile-optimized models like Gemma across mobile platforms</h2>
</div>
<div class="content">

  <div class="post-meta">

  <div>
    <i class="fa fa-calendar fa-fw"></i>
    <time>03 Jun 2025</time>
  </div>

  

  
  
  
  <div>
    <i class="fa fa-folder fa-fw"></i>
    
      <a class="post-taxonomy-topic" href="https://annjose.com/topics/tech-explorations">tech-explorations</a>&nbsp;&#47;
    
      <a class="post-taxonomy-topic" href="https://annjose.com/topics/on-device-ai">on-device-ai</a>&nbsp;&#47;
    
      <a class="post-taxonomy-topic" href="https://annjose.com/topics/edge-ai">edge-ai</a>
    
  </div>
  
  

  
  
  
  <div>
    <i class="fa fa-tags fa-fw"></i>
    
      <a class="post-taxonomy-tag" href="https://annjose.com/tags/mobile">mobile</a>&nbsp;&#47;
    
      <a class="post-taxonomy-tag" href="https://annjose.com/tags/AI">AI</a>&nbsp;&#47;
    
      <a class="post-taxonomy-tag" href="https://annjose.com/tags/mobile-development">mobile-development</a>&nbsp;&#47;
    
      <a class="post-taxonomy-tag" href="https://annjose.com/tags/privacy">privacy</a>&nbsp;&#47;
    
      <a class="post-taxonomy-tag" href="https://annjose.com/tags/gemma">gemma</a>&nbsp;&#47;
    
      <a class="post-taxonomy-tag" href="https://annjose.com/tags/hands-on">hands-on</a>
    
  </div>
  
  

</div>

  <p>In our previous post, &ldquo;<a href="https://annjose.com/post/mobile-ondevice-ai/">Mobile On-device AI: Smarter, Faster, and Private Apps</a>&rdquo;, we explored the fundamentals of running AI locally on mobile devices – what it is, its advantages, challenges, and the core concepts. Now, it&rsquo;s time to roll up our sleeves and see this technology in action!</p>
<p>This practical guide will walk you through getting hands-on with mobile on-device AI, focusing on how to run Google&rsquo;s powerful open-weight Gemma model family, including the latest Gemma 3n where supported, on both iOS and Android devices. We&rsquo;ll explore the tools and techniques to bring these advanced AI capabilities directly into your mobile applications.</p>
<h2 id="understanding-gemma-our-example-model-family">Understanding Gemma: Our Example Model Family</h2>
<p>Before we dive into the hands-on sections, let&rsquo;s briefly revisit the Gemma model family and its significance, particularly the newer Gemma 3n.</p>
<h3 id="gemini-nano-vs-gemma-3n">Gemini Nano vs. Gemma 3n</h3>
<p>Prior to the announcement of Gemma 3n at Google I/O 2025, prominent models for on-device AI included earlier Gemma versions and Gemini Nano. Both are optimized for on-device AI, though they cater to different use cases and have distinct access models. Here is a comparison of two models side-by-side:</p>



<table class="pure-table pure-table-striped">
  <thead><tr>
    
      <th>Feature                 </th>
    
      <th> Gemini Nano                                                                </th>
    
      <th> Gemma 3n</th>
    
  </thead></tr>
  <tbody>
  
    <tr>
      
      
        <td>Mobile Optimization </td>
      
        <td> Highly optimized for resource-constrained devices (e.g., &lt;4GB RAM devices) </td>
      
        <td> Optimized for medium-tier devices (e.g., phones/tablets with 4-8GB RAM)</td>
      
    </tr>
  
    <tr>
      
      
        <td>Device Support      </td>
      
        <td> Wearables, IoT, low-end Android phones                                     </td>
      
        <td> Android, iOS, tablets, laptops</td>
      
    </tr>
  
    <tr>
      
      
        <td>Model Access        </td>
      
        <td> Proprietary - Available via Android AICore/ML Kit GenAI APIs (restricted) </td>
      
        <td> Open weights - available at Kaggle Model Hub or LiteRT HuggingFace community</td>
      
    </tr>
  
    <tr>
      
      
        <td>Use Cases           </td>
      
        <td> On-device AI features in Pixel devices, Google Messages Smart Reply  </td>
      
        <td> Custom LLM apps, Cross-platform AI features, Local RAG implementations</td>
      
    </tr>
  
    <tr>
      
      
        <td>Feasibility         </td>
      
        <td> Limited to Google&#39;s integrated use cases                                     </td>
      
        <td> Designed for broader developer access</td>
      
    </tr>
  
  </tbody>
</table>
<h3 id="where-to-find-these-models">Where to Find These Models</h3>
<ul>
<li><strong>Gemma Models (including Gemma 3n and earlier versions):</strong>
<ul>
<li>Kaggle Model Hub: <a href="https://models.kaggle.com/google/gemma" target="_blank" rel="noopener">models.kaggle.com/google/gemma</a> (Official source for Gemma open models)</li>
<li>Hugging Face (LiteRT Community for Gemma 3n Preview): <a href="https://huggingface.co/collections/google/gemma-3n-preview-682ca41097a31e5ac804d57b" target="_blank" rel="noopener">huggingface.co/collections/google/gemma-3n-preview&hellip;</a> (For the Gemma 3n preview specifically optimized with LiteRT)</li>
</ul>
</li>
<li><strong>Gemini Nano:</strong>
<ul>
<li>Access for developers is typically indirect, through Android APIs like <strong>AICore</strong> or as part of <strong>ML Kit&rsquo;s GenAI features</strong>. Direct model download is not generally available due to its proprietary nature. (<a href="https://developer.android.com/ai/gemini-nano" target="_blank" rel="noopener">Learn more about Gemini Nano on Android</a>)</li>
</ul>
</li>
</ul>
<p>Despite their differences in access and target use cases, both model families stem from Google&rsquo;s advanced AI research. We will likely see continued evolution in their capabilities, how they are delivered, and the business models surrounding them for on-device applications. For this hands-on guide, we will primarily focus on the openly accessible Gemma models, particularly Gemma 3n, due to their suitability for broader developer experimentation.</p>
<h2 id="deep-dive-gemma-3n">Deep dive: Gemma 3n</h2>
<p>This is the latest open model optimized to run on mobile devices with at least 2GB RAM. It provides real-time AI on phones, tablets and laptops. It is in early preview and is available on Google AI Studio, Google AI Edge SDK and <a href="https://huggingface.co/collections/google/gemma-3n-preview-682ca41097a31e5ac804d57b" target="_blank" rel="noopener">LiteRT HuggingFace community</a>.</p>
<p>Gemma 3n can be deployed on mobile devices using the MediaPipe library of Google AI Edge SDK that we saw earlier. It is built on the same advanced architecture that powers <strong>Gemini Nano</strong>, the flagship mobile-optimized model. The development of Gemma 3n is a collaboration between Google DeepMind, Qualcomm, and Samsung, ensuring that it is engineered for high performance and efficiency across a wide range of mobile hardware.</p>
<blockquote>
<p>Gemma 3n is the mobile-optimized, multimodal model that enables developers to build mobile-first AI applications - everything runs locally.</p>
</blockquote>
<h3 id="key-features">Key Features</h3>
<ul>
<li><strong>Natively Multimodal:</strong> Supports text, image, audio, and video inputs and outputs</li>
<li><strong>Multi-Lingual:</strong> Offers robust support for multiple languages - trained over 140 languages</li>
<li><strong>Open Weights:</strong> The model is available under open weights, encouraging community development and transparency</li>
<li><strong>Realtime Capabilities:</strong> Enables real-time transcription, translation, and voice interactions directly on device</li>
<li><strong>Extended Context Length:</strong> Supports a context window of up to 32K tokens, significant input length for complex and lengthy data</li>
</ul>
<h3 id="model-variants">Model variants</h3>
<p>Two main variants are available in the preview:</p>
<ol>
<li><strong>Gemma 3n E2B:</strong> <a href="https://huggingface.co/google/gemma-3n-E2B-it-litert-preview" target="_blank" rel="noopener">HuggingFace Model Hub Link</a></li>
<li><strong>Gemma 3n E4B:</strong> <a href="https://huggingface.co/google/gemma-3n-E4B-it-litert-preview" target="_blank" rel="noopener">HuggingFace Model Hub Link</a></li>
</ol>
<p>For reference, the previous model <strong>Gemma 3-1B</strong> is available at <a href="https://huggingface.co/litert-community/Gemma3-1B-IT" target="_blank" rel="noopener">https://huggingface.co/litert-community/Gemma3-1B-IT</a></p>
<h3 id="naming-is-hard-but-fun-too">Naming is hard, but fun too!</h3>
<p>There are a few subtle elements in the name of the Gemma models that are interesting to know.</p>
<ul>
<li>The <strong>n</strong> suffix in the model name refers to the <em>nano-sized</em> models in the Gemma family, specifically optimized for resource-constrained devices like mobile phones and wearables</li>
<li>The <strong>E</strong> prefix in the Gemma 3n model names stands for the <em>Effective</em> parameter size, indicating the reduced, or effective, number of parameters the model actually needs to use during inference on a device, rather than the total parameter count stored in the model files. For example, the Gemma 3n E2B model contains 5 billion parameters, but it can operate with an effective memory footprint of 2B parameters because of the techniques like Per-Layer Embedding (PLE) caching and selective activation of parameters. Similarly the E4B model needs only 4B parameters to be active at run time even though it is a 8B parameter model.</li>
</ul>
<p>LiteRT is the new name for <em>TensorFlowLite</em> - and it stands for <em>Lite RunTime</em></p>
<h3 id="architecture-of-gemma-3n">Architecture of Gemma 3n</h3>
<p>There are a few important aspects of the new architecture that powers Gemma 3n and the Gemini Nano models.</p>
<ul>
<li><strong>PLE Caching:</strong> PLE (Per-Layer Embedding) parameters are generated separately and cached in fast storage, then added during inference, reducing the memory footprint at runtime</li>
<li><strong>Selective Parameter Activation:</strong> The model can dynamically load only the parameters needed for the current task (e.g., text, image, or audio), further reducing resource use. The model with 4B active memory footprint embeds a nested state-of-the-art submodel with 2B active memory footprint. This allows the model to dynamically load the relevant submodel to balance performance and quality based on device capabilities, user query etc.</li>
<li><strong>MatFormer Architecture:</strong> This allows the model to nest smaller submodels within a larger one, so only the relevant submodel is activated per request, minimizing compute and memory requirements</li>
</ul>
<h2 id="frameworks-in-action-google-ai-edge-and-core-ml">Frameworks in action: Google AI Edge and Core ML</h2>
<p>Once you have an optimized AI model (like the ones discussed in the Models section above), you need a way to actually run it on a mobile device. This is where on-device AI frameworks and runtimes come into play. They provide the necessary engine and tools to efficiently execute these models, manage resources, and often interact with device hardware for acceleration.</p>
<p>The two main players in the mobile ecosystem are:</p>
<ul>
<li>
<p><strong>Google AI Edge:</strong> Google&rsquo;s comprehensive suite for on-device AI - libraries, tools, and frameworks designed to deploy and run ML models directly on edge devices. Key components include:</p>
<ul>
<li><strong>LiteRT (formerly TensorFlow Lite):</strong> A crucial open-source runtime for executing a wide range of optimized models, including Google&rsquo;s Gemma family and custom-trained models, on Android, iOS, and other platforms. You can choose a model for your use case, convert it to the LiteRT format, and run it on-device</li>
<li><strong>MediaPipe:</strong> Offers pre-built, customizable solutions for tasks like vision and audio processing, often leveraging LiteRT. You can choose the task that solves your AI/ML problem, and implement it on multiple platforms. If you intend to use MediaPipe Tasks, refer to the <a href="https://ai.google.dev/edge/mediapipe/solutions/tasks" target="_blank" rel="noopener">MediaPipe Tasks documentation</a></li>
<li><strong>ML Kit:</strong> Provides easy-to-integrate APIs for common AI features, sometimes powered by models like Gemini Nano under the hood for specific functionalities within Google&rsquo;s ecosystem or via new GenAI APIs. See more info at <a href="https://developers.google.com/ml-kit" target="_blank" rel="noopener">ML Kit docs</a></li>
</ul>
</li>
<li>
<p><strong>Apple Core ML:</strong> Apple&rsquo;s dedicated framework for running machine learning models efficiently and privately on iOS, macOS, and other Apple platforms. It works with models converted to its specific format and optimizes execution across Apple&rsquo;s CPU, GPU, and Neural Engine.</p>
</li>
</ul>
<h2 id="deep-dive-apple-core-ml">Deep dive: Apple Core ML</h2>
<p>Core ML is Apple’s framework for running machine learning models on iOS, macOS, tvOS, and watchOS devices. It is designed for on-device inference, minimizing memory footprint and power consumption while keeping data private. It leverages Apple silicon, including the CPU, GPU, and Neural Engine, to maximize speed and minimize memory footprint and power consumption.</p>
<p>It works with Vision (for image and video analysis), Natural Language (for text processing), and GameplayKit (for game AI). Models trained in TensorFlow, PyTorch, or other frameworks can be converted to Core ML format using Core ML Tools, making it straightforward to deploy existing models on Apple devices.</p>
<h3 id="how-coreml-works">How CoreML works</h3>
<ul>
<li><strong>Model Preparation:</strong> ML models are trained on powerful computers and converted to the Core ML format (<code>.mlmodel</code>), optimized for Apple hardware - using <a href="https://apple.github.io/coremltools/docs-guides/source/introductory-quickstart.html" target="_blank" rel="noopener">Core ML Tools</a></li>
<li><strong>Integration:</strong> The <code>.mlmodel</code> file is added to an Xcode project. Xcode automatically generates a Swift or Objective-C class for the model (<code>.mlmodelc</code>), providing type-safe access to inputs and outputs</li>
<li><strong>Execution:</strong> The app passes input data (e.g., images, text) to the model. Core ML dispatches computations to the most appropriate hardware (CPU, GPU, or Neural Engine) for efficient inference</li>
<li><strong>Stateful Models:</strong> Recent Core ML updates support stateful models (like those used in generative AI), managing state (such as key-value caches for language models) to improve efficiency and reduce overhead</li>
<li><strong>Performance Profiling:</strong> You can can profile model performance, identify bottlenecks, and optimize models for specific hardware using tools in Xcode and Core ML Tools</li>
</ul>
<p>Here is how you can use a model using Core ML in your app. Note - <code>.mlmodelc</code> is the compiled version of the model you imported into XCode.</p>
<div class="highlight"><div style="color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">
<table style="border-spacing:0;padding:0;margin:0;border:0;"><tr><td style="vertical-align:top;padding:0;margin:0;border:0;">
<pre tabindex="0" style="color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 1
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 2
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 3
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 4
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 5
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 6
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 7
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 8
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 9
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">10
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">11
</span></code></pre></td>
<td style="vertical-align:top;padding:0;margin:0;border:0;;width:100%">
<pre tabindex="0" style="color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-swift" data-lang="swift"><span style="display:flex;"><span>	<span style="color:#ff79c6">guard</span> <span style="color:#8be9fd;font-style:italic">let</span> <span style="color:#8be9fd;font-style:italic">modelURL</span> = Bundle.main.url(
</span></span><span style="display:flex;"><span>                            forResource: <span style="color:#f1fa8c">&#34;CoreMLTrainedModel&#34;</span>, 
</span></span><span style="display:flex;"><span>                            withExtension: <span style="color:#f1fa8c">&#34;mlmodelc&#34;</span>) <span style="color:#ff79c6">else</span> {
</span></span><span style="display:flex;"><span>	    fatalError(<span style="color:#f1fa8c">&#34;Failed to locate the Core ML model file.&#34;</span>)
</span></span><span style="display:flex;"><span>	}
</span></span><span style="display:flex;"><span>	<span style="color:#ff79c6">do</span> {
</span></span><span style="display:flex;"><span>	    <span style="color:#8be9fd;font-style:italic">let</span> <span style="color:#8be9fd;font-style:italic">model</span> = <span style="color:#ff79c6">try</span> MLModel(contentsOf: modelURL)
</span></span><span style="display:flex;"><span>	    <span style="color:#6272a4">// Now you have Core ML model object ready to make predictions</span>
</span></span><span style="display:flex;"><span>	} <span style="color:#ff79c6">catch</span> {
</span></span><span style="display:flex;"><span>	    fatalError(<span style="color:#f1fa8c">&#34;Failed to create Core ML model object: </span><span style="color:#f1fa8c">\(</span>error<span style="color:#f1fa8c">)</span><span style="color:#f1fa8c">&#34;</span>)
</span></span><span style="display:flex;"><span>	}
</span></span></code></pre></td></tr></table>
</div>
</div><p>After this, you prepare your input data so it matches the requirements of your model. This might involve resizing images, normalizing values, or encoding text. For example, you may need to resize an image to a specific dimension before passing it to the model. Then, pass the prepared input data to the model’s prediction method. The model will process the input and generate an output. Once you receive the model’s output, interpret and use it as needed in your app. Finally, display the results to the user.</p>
<p>Read more about it at <a href="https://apple.github.io/coremltools/docs-guides/source/introductory-quickstart.html" target="_blank" rel="noopener">CoreML Quick Start Guide</a>.</p>
<h2 id="hands-on-running-gemma-models-on-mobile-devices">Hands-on: Running Gemma Models on mobile devices</h2>
<p>Ok, let’s roll up our sleeves and see these models in action. We can do that in a few places:</p>
<ol>
<li><strong>Google AI Studio:</strong> This is Google’s playground where you can try all their models in a chat-like interface. It’s the simplest way to see Gemma models, including the latest Gemma 3n.</li>
<li><strong>Sample apps from MediaPipe:</strong> These are available for Android, iOS, and web browsers. They let you run models directly on your device or in the browser.</li>
<li><strong>Google AI Edge Gallery app:</strong> This new app from Google AI Edge SDK lets you download and run models like Gemma 3n directly on your Android device, offline.</li>
</ol>
<p><strong>Note:</strong> The mobile setup is fairly easy. You don’t need advanced mobile expertise - just a compatible USB cable and the ability to follow instructions.</p>
<h3 id="1-run-gemma-on-google-ai-studio">1. Run Gemma on Google AI Studio</h3>
<p>This is the fastest way to try out Gemma 3n without any setup hassle.</p>
<ul>
<li>Go to <a href="https://aistudio.google.com/prompts/new_chat" target="_blank" rel="noopener">Google AI Studio</a> and login with your Google account</li>
<li>Select the <strong>Gemma 3n 4B</strong> model in the dropdown menu on the right.</li>
<li>Start chatting with the model right away. Adjust the parameters like temperature, TopP etc. as you wish.</li>
<li>Other Gemma models are also available here. You can use any of them.</li>
</ul>
<p>Here is a screenshot of Google AI Studio running Gemma 3n 4B. As you can see, the answer to my question about a new book on heart health didn&rsquo;t come out correctly. This is a limitation of smaller models.









    
        
        
        
        
        
        
        <figure style="margin-inline-start: 10px; margin-inline-end: 10px; margin-left: auto; margin-right: auto; max-width: 100%; text-align: center;">
            <a href="https://annjose.com/post/mobile-ondevice-ai-hands-on-gemma/gemma-3n-ai-studio.png" target="_blank">
                <img srcset="https://annjose.com/post/mobile-ondevice-ai-hands-on-gemma/gemma-3n-ai-studio_hueedcda6fc31da7c3452e64fcc76304a7_1699561_320x0_resize_box_3.png 320w, https://annjose.com/post/mobile-ondevice-ai-hands-on-gemma/gemma-3n-ai-studio_hueedcda6fc31da7c3452e64fcc76304a7_1699561_480x0_resize_box_3.png 480w, https://annjose.com/post/mobile-ondevice-ai-hands-on-gemma/gemma-3n-ai-studio_hueedcda6fc31da7c3452e64fcc76304a7_1699561_768x0_resize_box_3.png 768w, https://annjose.com/post/mobile-ondevice-ai-hands-on-gemma/gemma-3n-ai-studio_hueedcda6fc31da7c3452e64fcc76304a7_1699561_1024x0_resize_box_3.png 1024w, https://annjose.com/post/mobile-ondevice-ai-hands-on-gemma/gemma-3n-ai-studio_hueedcda6fc31da7c3452e64fcc76304a7_1699561_1500x0_resize_box_3.png 1500w"
                     sizes="(max-width: 320px) 90vw, (max-width: 480px) 85vw, (max-width: 768px) 80vw, (max-width: 1200px) 60vw, 50vw"
                     src="https://annjose.com/post/mobile-ondevice-ai-hands-on-gemma/gemma-3n-ai-studio.png"
                     alt=""  
                     style="max-width: 100%; height: auto;" loading="lazy">
            </a>
            
        </figure>

    
</p>
<h3 id="2-run-gemma-on-android">2. Run Gemma on Android</h3>
<p>We have two options to run Gemma models on Android:</p>
<h4 id="21-mediapipe-sample-app">2.1 MediaPipe Sample App</h4>
<p>This app demonstrates all supported Small Language Models (SLMs) like Gemma 2, Gemma 3, Microsoft Phi, and QWEN. It’s available on <a href="https://github.com/google-ai-edge/mediapipe-samples" target="_blank" rel="noopener">GitHub: google-ai-edge/mediapipe-samples</a>.
- <strong>Drawbacks:</strong> The sample app is minimal, somewhat outdated, and not as polished as newer offerings. Importantly, <strong>you cannot run Gemma 3n models in this app</strong>. Attempts to modify the source code to add Gemma 3n often result in errors like “Access denied. Please try again and grant the necessary permissions,” even after setting up permissions on Hugging Face. In my understanding, this suggests the app is explicitly restricted from running newer models.</p>
<p>Here are some screenshots of MediaPipe sample app running on my Pixel 8 Pro Android device:


<div class="pure-g">

  
  
  
  
  <div class="pure-u-1 pure-u-md-1-2">
    <div style="padding: 0 .2em">
      <img
        class="pure-img-responsive"
        src="media-pipe-android-app-models.png"
        alt="MediaPipe sample with Gemma 3 - 1B">
    </div>
  </div>
  

  
  
  
  
  <div class="pure-u-1 pure-u-md-1-2">
    <div style="padding: 0 .2em">
      <img
        class="pure-img-responsive"
        src="mediapipe-android-chat-gemma-3-1B.png"
        alt="MediaPipe sample with Gemma 3 - 1B">
    </div>
  </div>
  

</div>
</p>
<p>And the sample app running in Adroid studio









    
        
        
        
        
        
        
        <figure style="margin-inline-start: 10px; margin-inline-end: 10px; margin-left: auto; margin-right: auto; max-width: 100%; text-align: center;">
            <a href="https://annjose.com/post/mobile-ondevice-ai-hands-on-gemma/mediapipe-android-app-AndroidStudio.png" target="_blank">
                <img srcset="https://annjose.com/post/mobile-ondevice-ai-hands-on-gemma/mediapipe-android-app-AndroidStudio_hucb095717916dc5310ad011c7e75222bd_1263277_320x0_resize_box_3.png 320w, https://annjose.com/post/mobile-ondevice-ai-hands-on-gemma/mediapipe-android-app-AndroidStudio_hucb095717916dc5310ad011c7e75222bd_1263277_480x0_resize_box_3.png 480w, https://annjose.com/post/mobile-ondevice-ai-hands-on-gemma/mediapipe-android-app-AndroidStudio_hucb095717916dc5310ad011c7e75222bd_1263277_768x0_resize_box_3.png 768w, https://annjose.com/post/mobile-ondevice-ai-hands-on-gemma/mediapipe-android-app-AndroidStudio_hucb095717916dc5310ad011c7e75222bd_1263277_1024x0_resize_box_3.png 1024w, https://annjose.com/post/mobile-ondevice-ai-hands-on-gemma/mediapipe-android-app-AndroidStudio_hucb095717916dc5310ad011c7e75222bd_1263277_1500x0_resize_box_3.png 1500w"
                     sizes="(max-width: 320px) 90vw, (max-width: 480px) 85vw, (max-width: 768px) 80vw, (max-width: 1200px) 60vw, 50vw"
                     src="https://annjose.com/post/mobile-ondevice-ai-hands-on-gemma/mediapipe-android-app-AndroidStudio.png"
                     alt=""  
                     style="max-width: 100%; height: auto;" loading="lazy">
            </a>
            
        </figure>

    
</p>
<h4 id="22-google-ai-edge-gallery-app">2.2 Google AI Edge Gallery App</h4>
<p>This is a newer, more versatile sample app available as an experimental Alpha release. It’s sleek, easy to use, and demonstrates the full power of on-device AI. You can run Gemma 3n models (E2B and E4B) directly on your device, but not older models. <br>
- <strong>Download and instructions:</strong> <a href="https://github.com/google-ai-edge/gallery" target="_blank" rel="noopener">GitHub: google-ai-edge/gallery</a>
- <strong>How it works:</strong> After installing, select a model from the list (such as Gemma 3n E2B or E4B), and you can start using AI features like chat, image analysis, and more - all offline</p>
<p>Here are some screenshots of Gallery app running on my Pixel 8 Pro Android device - the home page that shows the tasks it supports, asking the user to review the license of Gemma 3n and downloading the Gemma 3n model:</p>


<div class="pure-g">

  
  
  
  
  <div class="pure-u-1 pure-u-md-1-3">
    <div style="padding: 0 .2em">
      <img
        class="pure-img-responsive"
        src="gallery-app-home.png"
        alt="Edge Gallery App Home">
    </div>
  </div>
  

  
  
  
  
  <div class="pure-u-1 pure-u-md-1-3">
    <div style="padding: 0 .2em">
      <img
        class="pure-img-responsive"
        src="gallery-app-hf-license.png"
        alt="Asking review license">
    </div>
  </div>
  

  
  
  
  
  <div class="pure-u-1 pure-u-md-1-3">
    <div style="padding: 0 .2em">
      <img
        class="pure-img-responsive"
        src="gallery-app-downloading.png"
        alt="Downloading the model">
    </div>
  </div>
  

</div>

<p>The gallery app supports three three main type of tasks - Chat, Explain Image and Summarize.


<div class="pure-g">

  
  
  
  
  <div class="pure-u-1 pure-u-md-1-3">
    <div style="padding: 0 .2em">
      <img
        class="pure-img-responsive"
        src="gemma-3n-chat-google-edge-gallery.png"
        alt="Chat using Gemma 3n">
    </div>
  </div>
  

  
  
  
  
  <div class="pure-u-1 pure-u-md-1-3">
    <div style="padding: 0 .2em">
      <img
        class="pure-img-responsive"
        src="gemma-3n-explain-image-google-edge-gallery.png"
        alt="Explain Image using Gemma 3n">
    </div>
  </div>
  

  
  
  
  
  <div class="pure-u-1 pure-u-md-1-3">
    <div style="padding: 0 .2em">
      <img
        class="pure-img-responsive"
        src="gemma-3n-summarize-google-edge-gallery.png"
        alt="Summarize using Gemma 3n">
    </div>
  </div>
  

</div>
</p>
<p>See detailed instructions for each app:</p>
<ul>
<li><strong>MediaPipe Sample App:</strong> <a href="https://github.com/google-ai-edge/mediapipe-samples" target="_blank" rel="noopener">MediaPipe Samples GitHub</a></li>
<li><strong>Gallery App:</strong> <a href="https://github.com/google-ai-edge/gallery" target="_blank" rel="noopener">Google AI Edge Gallery GitHub</a>
In summary, use the old MediaPipe sample app to run models prior to Gemma 3n, and the new Gallery app to run Gemma 3n models. Google may consolidate these apps in the future or deprecate older models.</li>
</ul>
<h3 id="3-run-gemma-on-ios">3. Run Gemma on iOS</h3>
<p><strong>The new Google AI Edge Gallery app is not yet available for iOS - its release is marked as “coming soon.”</strong><br>
Currently, our only option is to use the old MediaPipe sample app, which supports models prior to Gemma 3n (like Gemma 2 and Gemma 3).</p>
<p>The LLM Inference API from MediaPipe enables you to run large language models (LLMs) entirely on-device for iOS applications. This allows you to perform advanced AI tasks such as text generation, information retrieval, and document summarization - all while keeping user data private and secure.</p>
<p><strong>How to get started:</strong></p>
<ol>
<li><strong>Install the MediaPipeTasksGenai library</strong>  - Add the necessary dependencies to your project using CocoaPods. Add the following to your <code>Podfile</code>:</li>
</ol>
<div class="highlight"><div style="color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">
<table style="border-spacing:0;padding:0;margin:0;border:0;"><tr><td style="vertical-align:top;padding:0;margin:0;border:0;">
<pre tabindex="0" style="color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">1
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">2
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">3
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">4
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">5
</span></code></pre></td>
<td style="vertical-align:top;padding:0;margin:0;border:0;;width:100%">
<pre tabindex="0" style="color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-ruby" data-lang="ruby"><span style="display:flex;"><span>target <span style="color:#f1fa8c">&#39;MyLlmInferenceApp&#39;</span> <span style="color:#ff79c6">do</span>
</span></span><span style="display:flex;"><span>  use_frameworks!
</span></span><span style="display:flex;"><span>  pod <span style="color:#f1fa8c">&#39;MediaPipeTasksGenAI&#39;</span>
</span></span><span style="display:flex;"><span>  pod <span style="color:#f1fa8c">&#39;MediaPipeTasksGenAIC&#39;</span>
</span></span><span style="display:flex;"><span><span style="color:#ff79c6">end</span>
</span></span></code></pre></td></tr></table>
</div>
</div><p>Note - The library is compatible with both Swift and Objective-C apps and does not require any additional language-specific setup.</p>
<ol start="2">
<li>
<p><strong>Download a compatible model</strong>   - Download a supported model (such as Gemma-2 2B in 8-bit quantized format) from Kaggle Models. Add the model file to your Xcode project.</p>
</li>
<li>
<p><strong>Initialize the LLM Inference task</strong>  - Configure the task with your model path and desired options:</p>
</li>
</ol>
<div class="highlight"><div style="color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">
<table style="border-spacing:0;padding:0;margin:0;border:0;"><tr><td style="vertical-align:top;padding:0;margin:0;border:0;">
<pre tabindex="0" style="color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 1
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 2
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 3
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 4
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 5
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 6
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 7
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 8
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 9
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">10
</span></code></pre></td>
<td style="vertical-align:top;padding:0;margin:0;border:0;;width:100%">
<pre tabindex="0" style="color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-swift" data-lang="swift"><span style="display:flex;"><span><span style="color:#8be9fd;font-style:italic">import</span> <span style="color:#50fa7b">MediaPipeTasksGenai</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#8be9fd;font-style:italic">let</span> <span style="color:#8be9fd;font-style:italic">modelPath</span> = Bundle.main.path(forResource: <span style="color:#f1fa8c">&#34;model&#34;</span>, ofType: <span style="color:#f1fa8c">&#34;bin&#34;</span>)
</span></span><span style="display:flex;"><span><span style="color:#8be9fd;font-style:italic">let</span> <span style="color:#8be9fd;font-style:italic">options</span> = LlmInferenceOptions()
</span></span><span style="display:flex;"><span>options.baseOptions.modelPath = modelPath
</span></span><span style="display:flex;"><span>options.maxTokens = <span style="color:#bd93f9">1000</span>
</span></span><span style="display:flex;"><span>options.topk = <span style="color:#bd93f9">40</span>
</span></span><span style="display:flex;"><span>options.temperature = <span style="color:#bd93f9">0.8</span>
</span></span><span style="display:flex;"><span>options.randomSeed = <span style="color:#bd93f9">101</span>
</span></span><span style="display:flex;"><span><span style="color:#8be9fd;font-style:italic">let</span> <span style="color:#8be9fd;font-style:italic">llmInference</span> = <span style="color:#ff79c6">try</span> LlmInference(options: options)
</span></span></code></pre></td></tr></table>
</div>
</div><ol start="4">
<li><strong>Run the task and generate responses</strong>  - Use the <code>generateResponse(inputText:)</code> method for single responses, or <code>generateResponseAsync(inputText:)</code> for streaming:</li>
</ol>
<div class="highlight"><div style="color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">
<table style="border-spacing:0;padding:0;margin:0;border:0;"><tr><td style="vertical-align:top;padding:0;margin:0;border:0;">
<pre tabindex="0" style="color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 1
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 2
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 3
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 4
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 5
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 6
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 7
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 8
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 9
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">10
</span></code></pre></td>
<td style="vertical-align:top;padding:0;margin:0;border:0;;width:100%">
<pre tabindex="0" style="color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-swift" data-lang="swift"><span style="display:flex;"><span><span style="color:#8be9fd;font-style:italic">let</span> <span style="color:#8be9fd;font-style:italic">result</span> = <span style="color:#ff79c6">try</span> LlmInference.generateResponse(inputText: inputPrompt)
</span></span><span style="display:flex;"><span><span style="color:#6272a4">// For streaming:</span>
</span></span><span style="display:flex;"><span><span style="color:#8be9fd;font-style:italic">let</span> <span style="color:#8be9fd;font-style:italic">resultStream</span> = LlmInference.generateResponseAsync(inputText: inputPrompt)
</span></span><span style="display:flex;"><span><span style="color:#ff79c6">do</span> {
</span></span><span style="display:flex;"><span>  <span style="color:#ff79c6">for</span> <span style="color:#ff79c6">try</span> await partialResult <span style="color:#ff79c6">in</span> resultStream {
</span></span><span style="display:flex;"><span>    print(<span style="color:#f1fa8c">&#34;</span><span style="color:#f1fa8c">\(</span>partialResult<span style="color:#f1fa8c">)</span><span style="color:#f1fa8c">&#34;</span>)
</span></span><span style="display:flex;"><span>  }
</span></span><span style="display:flex;"><span>} <span style="color:#ff79c6">catch</span> {
</span></span><span style="display:flex;"><span>  print(<span style="color:#f1fa8c">&#34;Response error: </span><span style="color:#f1fa8c">\(</span>error<span style="color:#f1fa8c">)</span><span style="color:#f1fa8c">&#34;</span>)
</span></span><span style="display:flex;"><span>}
</span></span></code></pre></td></tr></table>
</div>
</div><ol start="5">
<li><strong>More configuration</strong> - You can adjust the model parameters like <strong>maxTokens</strong>, <strong>topk</strong>, <strong>temperature</strong>, <strong>randomSeed</strong> and path to the model file as <strong>modelPath</strong></li>
<li><strong>Sample application</strong> - The <a href="https://github.com/google-ai-edge/mediapipe-samples" target="_blank" rel="noopener">MediaPipe Samples GitHub repository</a> includes a sample iOS app that demonstrates how to use the LLM Inference API. Clone the repository and open the project in Xcode to see it in action.</li>
</ol>
<p>The API also supports LoRA (Low-Rank Adaptation) for efficient fine-tuning of models, allowing you to customize model behavior for specific tasks.</p>
<p>For detailed instructions and advanced setup, see the official documentation: <a href="https://ai.google.dev/edge/mediapipe/solutions/genai/llm_inference/ios" target="_blank" rel="noopener">MediaPipe LLM Inference API for iOS</a></p>
<h3 id="4-bonus-run-gemma-on-mobile-browsers">4. Bonus: Run Gemma on mobile browsers</h3>
<p>You can run Gemma and other large language models directly in your browser using the MediaPipe LLM Inference API for JavaScript. This allows you to perform advanced AI tasks - such as text generation, information retrieval, and document summarization - entirely on-device, without sending data to a server.</p>
<p><strong>Supported Models</strong></p>
<ul>
<li><strong>Gemma-2 2B</strong>, <strong>Gemma-3 1B</strong>, and other models like Phi-2 and StableLM are supported.</li>
<li><strong>Gemma 3n (E2B, E4B)</strong> is not currently listed as supported for the Web API.</li>
</ul>
<p>It is fascinating to see the model loaded into the browser on your machine. Of course, tools like Ollama does the same, but it feels different when you see that happen in the context of a browser - which you usually think of as a lightweight window to the world wide web.</p>
<p><strong>Setup instructions</strong></p>
<ul>
<li><strong>Install the package:</strong></li>
</ul>
<div class="highlight"><div style="color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">
<table style="border-spacing:0;padding:0;margin:0;border:0;"><tr><td style="vertical-align:top;padding:0;margin:0;border:0;">
<pre tabindex="0" style="color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">1
</span></code></pre></td>
<td style="vertical-align:top;padding:0;margin:0;border:0;;width:100%">
<pre tabindex="0" style="color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-bash" data-lang="bash"><span style="display:flex;"><span>    npm install @mediapipe/tasks-genai
</span></span></code></pre></td></tr></table>
</div>
</div><p>Or use the CDN:</p>
<div class="highlight"><div style="color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">
<table style="border-spacing:0;padding:0;margin:0;border:0;"><tr><td style="vertical-align:top;padding:0;margin:0;border:0;">
<pre tabindex="0" style="color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">1
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">2
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">3
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">4
</span></code></pre></td>
<td style="vertical-align:top;padding:0;margin:0;border:0;;width:100%">
<pre tabindex="0" style="color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-xml" data-lang="xml"><span style="display:flex;"><span>	<span style="color:#ff79c6">&lt;script</span> 
</span></span><span style="display:flex;"><span>        <span style="color:#50fa7b">src=</span><span style="color:#f1fa8c">&#34;https://cdn.jsdelivr.net/npm/@mediapipe/tasks-genai/genai_bundle.cjs&#34;</span> 
</span></span><span style="display:flex;"><span>        <span style="color:#50fa7b">crossorigin=</span><span style="color:#f1fa8c">&#34;anonymous&#34;</span><span style="color:#ff79c6">&gt;</span>
</span></span><span style="display:flex;"><span>    <span style="color:#ff79c6">&lt;/script&gt;</span>
</span></span></code></pre></td></tr></table>
</div>
</div><ul>
<li>
<p><strong>Download a model:</strong>  For example, download Gemma-2 2B in 8-bit quantized format from Kaggle Models or HuggingFace LiteRT and place it in your project directory.</p>
</li>
<li>
<p><strong>Initialize the task:</strong></p>
</li>
</ul>
<div class="highlight"><div style="color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">
<table style="border-spacing:0;padding:0;margin:0;border:0;"><tr><td style="vertical-align:top;padding:0;margin:0;border:0;">
<pre tabindex="0" style="color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 1
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 2
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 3
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 4
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 5
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 6
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 7
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 8
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 9
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">10
</span></code></pre></td>
<td style="vertical-align:top;padding:0;margin:0;border:0;;width:100%">
<pre tabindex="0" style="color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-javascript" data-lang="javascript"><span style="display:flex;"><span>    <span style="color:#ff79c6">const</span> genai <span style="color:#ff79c6">=</span> <span style="color:#ff79c6">await</span> FilesetResolver.forGenAiTasks(
</span></span><span style="display:flex;"><span>	  <span style="color:#f1fa8c">&#34;https://cdn.jsdelivr.net/npm/@mediapipe/tasks-genai@latest/wasm&#34;</span>
</span></span><span style="display:flex;"><span>	);
</span></span><span style="display:flex;"><span>	llmInference <span style="color:#ff79c6">=</span> <span style="color:#ff79c6">await</span> LlmInference.createFromOptions(genai, {
</span></span><span style="display:flex;"><span>	  baseOptions<span style="color:#ff79c6">:</span> { modelAssetPath<span style="color:#ff79c6">:</span> <span style="color:#f1fa8c">&#39;/assets/gemma-2b-it-gpu-int8.bin&#39;</span> },
</span></span><span style="display:flex;"><span>	  maxTokens<span style="color:#ff79c6">:</span> <span style="color:#bd93f9">1000</span>,
</span></span><span style="display:flex;"><span>	  topK<span style="color:#ff79c6">:</span> <span style="color:#bd93f9">40</span>,
</span></span><span style="display:flex;"><span>	  temperature<span style="color:#ff79c6">:</span> <span style="color:#bd93f9">0.8</span>,
</span></span><span style="display:flex;"><span>	  randomSeed<span style="color:#ff79c6">:</span> <span style="color:#bd93f9">101</span>
</span></span><span style="display:flex;"><span>	});
</span></span></code></pre></td></tr></table>
</div>
</div><ul>
<li><strong>Run inference:</strong></li>
</ul>
<div class="highlight"><div style="color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">
<table style="border-spacing:0;padding:0;margin:0;border:0;"><tr><td style="vertical-align:top;padding:0;margin:0;border:0;">
<pre tabindex="0" style="color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">1
</span></code></pre></td>
<td style="vertical-align:top;padding:0;margin:0;border:0;;width:100%">
<pre tabindex="0" style="color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-javascript" data-lang="javascript"><span style="display:flex;"><span>	<span style="color:#ff79c6">const</span> response <span style="color:#ff79c6">=</span> <span style="color:#ff79c6">await</span> llmInference.generateResponse(inputPrompt);
</span></span></code></pre></td></tr></table>
</div>
</div><p>Or stream the output:</p>
<div class="highlight"><div style="color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">
<table style="border-spacing:0;padding:0;margin:0;border:0;"><tr><td style="vertical-align:top;padding:0;margin:0;border:0;">
<pre tabindex="0" style="color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">1
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">2
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">3
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">4
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">5
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">6
</span></code></pre></td>
<td style="vertical-align:top;padding:0;margin:0;border:0;;width:100%">
<pre tabindex="0" style="color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-javascript" data-lang="javascript"><span style="display:flex;"><span>	llmInference.generateResponse(
</span></span><span style="display:flex;"><span>        inputPrompt,
</span></span><span style="display:flex;"><span>        (partialResult, done) =&gt; {
</span></span><span style="display:flex;"><span>            <span style="color:#8be9fd;font-style:italic">document</span>.getElementById(<span style="color:#f1fa8c">&#39;output&#39;</span>).textContent <span style="color:#ff79c6">+=</span> partialResult;
</span></span><span style="display:flex;"><span>        }
</span></span><span style="display:flex;"><span>    );
</span></span></code></pre></td></tr></table>
</div>
</div><ul>
<li><strong>Browser Compatibility</strong> - Requires WebGPU support. Check the <a href="https://ai.google.dev/edge/mediapipe/solutions/genai/llm_inference/web_js#gpu_browser_compatibility" target="_blank" rel="noopener">GPU browser compatibility</a> page for details.</li>
<li><strong>LoRA (Low-Rank Adaptation)</strong> - You can fine-tune models using LoRA and load custom LoRA weights at runtime for specialized tasks.</li>
</ul>
<p>See detailed instructions: <a href="https://ai.google.dev/edge/mediapipe/solutions/genai/llm_inference/web_js" target="_blank" rel="noopener">MediaPipe LLM Inference API for Web</a></p>
<h3 id="cross-platform-summary">Cross platform summary</h3>
<p>The following table summarizes how to run the model on various platforms.</p>



<table class="pure-table pure-table-striped">
  <thead><tr>
    
      <th>Platform</th>
    
      <th>Supported Gemma Models</th>
    
      <th>Recommended App/Interface</th>
    
      <th>Notes</th>
    
  </thead></tr>
  <tbody>
  
    <tr>
      
      
        <td>Google AI Studio</td>
      
        <td>Gemma 3n, others</td>
      
        <td>Web browser</td>
      
        <td>Easiest, no setup required</td>
      
    </tr>
  
    <tr>
      
      
        <td>Android</td>
      
        <td>Prior to Gemma 3n</td>
      
        <td>MediaPipe Sample App (GitHub)</td>
      
        <td>Outdated, not Gemma 3n</td>
      
    </tr>
  
    <tr>
      
      
        <td>Android</td>
      
        <td>Gemma 3n</td>
      
        <td>Google AI Edge Gallery App</td>
      
        <td>New, sleek, supports Gemma 3n</td>
      
    </tr>
  
    <tr>
      
      
        <td>iOS</td>
      
        <td>Prior to Gemma 3n</td>
      
        <td>MediaPipe Sample App (GitHub)</td>
      
        <td>No Gemma 3n, Gallery app coming soon</td>
      
    </tr>
  
    <tr>
      
      
        <td>Web browser</td>
      
        <td>Prior to Gemma 3n</td>
      
        <td>MediaPipe LLM Inference API (JS)</td>
      
        <td>Runs in browser, not Gemma 3n</td>
      
    </tr>
  
  </tbody>
</table>
<h2 id="conclusion">Conclusion</h2>
<p>Getting hands-on with on-device AI, especially with powerful open models like the Gemma family, is more accessible than ever for mobile developers. By leveraging tools like Google AI Edge (with MediaPipe and LiteRT) and understanding native frameworks like Apple Core ML, you can build truly innovative, private, and responsive AI features into your iOS and Android applications.</p>
<p>The landscape is evolving rapidly, particularly with models like Gemma 3n, opening up exciting possibilities for the future of mobile AI. We encourage you to experiment with the sample apps and start thinking about how you can integrate these capabilities into your own projects.</p>
<h2 id="references">References</h2>
<p><strong>Google On-Device AI &amp; Models:</strong></p>
<ul>
<li>Announcement of Gemma 3n at Google I/O 2025: <a href="https://developers.googleblog.com/en/introducing-gemma-3n/" target="_blank" rel="noopener">Google Developers Blog: Introducing Gemma 3n</a></li>
<li>Google AI Guide for Android: <a href="https://developer.android.com/ai/overview" target="_blank" rel="noopener">developer.android.com/ai/overview</a></li>
<li>Gemini Nano for Android: <a href="https://developer.android.com/ai/gemini-nano" target="_blank" rel="noopener">developer.android.com/ai/gemini-nano</a></li>
<li>Gemma Models Official Page: <a href="https://ai.google.dev/gemma" target="_blank" rel="noopener">ai.google.dev/gemma</a></li>
<li>TensorFlow Lite Guides: <a href="https://www.tensorflow.org/lite/guide" target="_blank" rel="noopener">tensorflow.org/lite/guide</a></li>
<li>ML Kit: <a href="https://developers.google.com/ml-kit" target="_blank" rel="noopener">developers.google.com/ml-kit</a></li>
<li>MediaPipe Solutions: <a href="https://ai.google.dev/edge/mediapipe/solutions" target="_blank" rel="noopener">ai.google.dev/edge/mediapipe/solutions</a></li>
</ul>
<p><strong>Apple On-Device AI:</strong></p>
<ul>
<li>Core ML Overview: <a href="https://developer.apple.com/documentation/coreml" target="_blank" rel="noopener">developer.apple.com/documentation/coreml</a></li>
<li>Core ML Tools (for model conversion): <a href="https://apple.github.io/coremltools/docs-guides/source/introductory-quickstart.html" target="_blank" rel="noopener">apple.github.io/coremltools</a> (Your link is good)</li>
</ul>


  
<div class="prev-next-post pure-g">
  <div class="pure-u-1-24" style="text-align: left;">
    
    <a href="https://annjose.com/post/mobile-on-device-ai/"><i class="fa fa-chevron-left"></i></a>
    
  </div>
  <div class="pure-u-10-24">
    
    <nav class="prev">
      <a href="https://annjose.com/post/mobile-on-device-ai/">Mobile On-device AI: Smarter, Faster, and Private Apps</a>
    </nav>
    
  </div>
  <div class="pure-u-2-24">
    &nbsp;
  </div>
  <div class="pure-u-10-24">
    
  </div>
  <div class="pure-u-1-24" style="text-align: right;">
    
  </div>
</div>



  
<div id="disqus_thread"></div>
<script type="text/javascript">

(function() {
    
    
    if (window.location.hostname == "localhost")
        return;

    var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
    var disqus_shortname = 'anncjose';
    dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
    (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
})();
</script>
<noscript>Please enable JavaScript to view the <a href="http://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
<a href="http://disqus.com/" class="dsq-brlink">comments powered by <span class="logo-disqus">Disqus</span></a>


</div>

</div>
</div>
<script src="https://annjose.com/js/ui.js"></script>
<script type="text/javascript"
  src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>

<script type="text/x-mathjax-config">
MathJax.Hub.Config({
  tex2jax: {
    inlineMath: [['$','$'], ['\\(','\\)']],
    displayMath: [['$$','$$'], ['\[','\]']],
    processEscapes: true,
    processEnvironments: true,
    skipTags: ['script', 'noscript', 'style', 'textarea', 'pre'],
    TeX: { equationNumbers: { autoNumber: "AMS" },
         extensions: ["AMSmath.js", "AMSsymbols.js"] }
  }
});
</script>




<script async src="https://www.googletagmanager.com/gtag/js?id=G-2PEL4BVYJE"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag() { dataLayer.push(arguments); }
  gtag('js', new Date());

  gtag('config', 'G-2PEL4BVYJE');
</script>




<script 
    id="counterscale-script" 
    data-site-id="annjose-blog"
    src="https://counterscale.annjose.workers.dev/tracker.js" 
    defer>
</script>

</body>
</html>

