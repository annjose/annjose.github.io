<!DOCTYPE html>
<html lang="en">

<head>
  <meta http-equiv="content-type" content="text/html; charset=utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <meta name="description"
    content="Comprehensive overview and hands-on guide to run AI models locally on your device">
  <meta name="generator" content="Hugo 0.121.1">

  <title>Mobile On-device AI &middot; Reflections</title>

  
  
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/pure/1.0.0/pure-min.css">

  <!--[if lte IE 8]>
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/pure/1.0.0/grids-responsive-old-ie-min.css">
  <![endif]-->
  <!--[if gt IE 8]><!-->
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/pure/1.0.0/grids-responsive-min.css">
  <!--<![endif]-->

  <!--[if lte IE 8]>
  <link rel="stylesheet" href="https://annjose.com/css/side-menu-old-ie.css">
  <![endif]-->
  <!--[if gt IE 8]><!-->
  <link rel="stylesheet" href="https://annjose.com/css/side-menu.css">
  <!--<![endif]-->

  <link rel="stylesheet" href="https://annjose.com/css/blackburn.css">

  
  
  <script src="https://kit.fontawesome.com/3e898cf239.js" crossorigin="anonymous"></script>
  
  
  <link rel="preconnect" href="https://fonts.googleapis.com">
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>

  
  

  
  
  

  
  

  

  <link rel="shortcut icon" href="https://annjose.com/img/favicon.ico" type="image/x-icon" />

  
  

</head>

<body>
<div id="layout">

  
<a href="#menu" id="menuLink" class="menu-link">
  
  <span></span>
</a>
<div id="menu">

  
  <a class="pure-menu-heading brand" href="https://annjose.com/">Reflections</a>


  <div class="pure-menu">
    <ul class="pure-menu-list">
      
      
        <li class="pure-menu-item">
          <a class="pure-menu-link" href="https://annjose.com/"><i class='fas fa-home fa-fw'></i>Home</a>
      
        </li>
      
      
        <li class="pure-menu-item">
          <a class="pure-menu-link" href="https://annjose.com/post/"><i class='fas fa-pen-to-square fa-fw'></i>My Blog</a>
      
        </li>
      
      
        <li class="pure-menu-item">
          <a class="pure-menu-link" href="https://annjose.com/tags/LLM"><i class='fas fa-hexagon-nodes fa-fw'></i>AI &amp; LLMs</a>
      
        </li>
      
      
        <li class="pure-menu-item">
          <a class="pure-menu-link" href="https://annjose.com/tags/tech-explorations"><i class='fas fa-magnifying-glass fa-fw'></i>Tech Explorations</a>
      
        </li>
      
      
        <li class="pure-menu-item">
          <a class="pure-menu-link" href="https://annjose.com/tags/personal-growth"><i class='fas fa-arrow-up-right-dots fa-fw'></i>Personal Growth</a>
      
        </li>
      
      
        <li class="pure-menu-item">
          <a class="pure-menu-link" href="https://annjose.com/tags/web-development"><i class='fas fa-globe fa-fw'></i>Web Development</a>
      
        </li>
      
      
        <li class="pure-menu-item">
          <a class="pure-menu-link" href="https://annjose.com/tags/mobile-tech"><i class='fas fa-mobile-screen fa-fw'></i>Mobile Tech</a>
      
        </li>
      
      
        <li class="pure-menu-item">
          <a class="pure-menu-link" href="https://annjose.com/about/"><i class='fas fa-user fa-fw'></i>About Me</a>
      
        </li>
      
    </ul>
  </div>

  <div class="pure-menu social">
  <ul class="pure-menu-list">

    

    

    
    <li class="pure-menu-item">
      <a class="pure-menu-link" href="https://bsky.app/profile/annjose.com" target="_blank"><i class="fa-brands fa-square-bluesky fa-fw"></i>Bluesky</a>
    </li>
    

    

    

    

    

    

    

    

    

    

    

    

    

    

    

    

    

    
    <li class="pure-menu-item">
      <a class="pure-menu-link" href="https://linkedin.com/in/annjose" target="_blank"><i class="fa fa-linkedin-square fa-fw"></i>LinkedIn</a>
    </li>
    

    

    

    

    
    <li class="pure-menu-item">
      <a class="pure-menu-link" href="https://news.ycombinator.com/user?id=annjose" target="_blank"><i class="fa fa-hacker-news fa-fw"></i>Hacker News</a>
    </li>
    

    

    
    <li class="pure-menu-item">
      <a class="pure-menu-link" href="https://github.com/annjose" target="_blank"><i class="fa fa-github-square fa-fw"></i>GitHub</a>
    </li>
    

    

    

    

    

    

    

    

    

    

    

    

    

  </ul>
</div>


  <div>
  <div class="small-print">
    <small>&copy; 2025. All rights reserved.</small>
  </div>
  <div class="small-print">
    <small>Built with<a href="https://gohugo.io/" target="_blank">Hugo</a></small>
  </div>
</div>

</div>


  <div id="main">


<div class="header">
  <h1>Mobile On-device AI</h1>
  <h2>Comprehensive overview and hands-on guide to run AI models locally on your device</h2>
</div>
<div class="content">

  <div class="post-meta">

  <div>
    <i class="fa fa-calendar fa-fw"></i>
    <time>31 May 2025</time>
  </div>

  

  
  
  
  <div>
    <i class="fa fa-folder fa-fw"></i>
    
      <a class="post-taxonomy-topic" href="https://annjose.com/topics/tech-explorations">tech-explorations</a>&nbsp;&#47;
    
      <a class="post-taxonomy-topic" href="https://annjose.com/topics/on-device-ai">on-device-ai</a>
    
  </div>
  
  

  
  
  
  <div>
    <i class="fa fa-tags fa-fw"></i>
    
      <a class="post-taxonomy-tag" href="https://annjose.com/tags/mobile">mobile</a>&nbsp;&#47;
    
      <a class="post-taxonomy-tag" href="https://annjose.com/tags/AI">AI</a>
    
  </div>
  
  

</div>

  <p>A lot of the buzz around AI is about big cloud models, but I’ve often wondered what we can do with AI right on our phones and tablets - making things more private, fast, and user-friendly. As someone who is passionate about mobile development and has been tinkering a lot with AI recently, I am fascinated by what happens when mobile development and AI come together - especially the possibilities of running AI directly on the device. I’ve been following this space on the side, but was excited by last week’s <a href="https://developers.googleblog.com/en/introducing-gemma-3n/" target="_blank" rel="noopener">Google’s announcement of Gemma 3n</a>, the latest efficient, cutting-edge model for Edge AI. So I decided to dig deeper.</p>
<p>But as I started exploring this space more, I noticed how fragmented the information is. There are a lot of announcements and high-level overviews, but it&rsquo;s hard to find practical, step-by-step guides for mobile on-device AI. That’s what inspired me to write this post - to consolidate my findings and provide a straightforward, actionable resource for anyone interested in running AI models locally on mobile.</p>
<p>In this post, I’ll explain the importance of mobile on-device AI, its advantages and main challenges (such as model size and device constraints), and the best tools and frameworks available today - including small LLMs, mobile AI frameworks like Google AI Edge, Apple Core ML and other alternatives. I’ll also include code examples and practical advice for implementing these solutions. As always, the best way to learn is by doing, so you will find hands-on examples here.</p>
<p>Whether you’re a beginner or a seasoned mobile developer, I hope this post helps you get an overview of mobile on-device AI and get started quickly if you wish. If you found this post useful, let me know through Bluesky (TBD add link here) and LinkedIn (TBD add link here), so you can share your thoughts and feedback there. If enough people are interested, I’ll write follow-up posts on how it works under the hood and more specific information on the frameworks for different mobile platforms.</p>
<p>Let&rsquo;s dive in!</p>
<h2 id="on-device-ai---an-overview">On-device AI - an Overview</h2>
<p>On-device AI (also known as Edge AI) is all about tools and technologies that allow you to run AI models directly on the device like your phone, smartwatch, or any IoT gadget - instead of sending data to the cloud. With Edge AI, you download the model to the device, and all the heavy lifting - running the model, crunching the data, and generating responses - happens right there on your hardware. Best of all, your data never leaves your device.</p>
<p>In contrast, Cloud AI works by running models on powerful servers in the cloud. Your device sends a request to those servers, the server does the computation, and then sends the result back to your device for you to see.</p>
<p>Let&rsquo;s take an example. Apps that summarize personal chats, emails, notes, and reminders often handle sensitive information, making on-device processing a must for user privacy. Similarly, summarization tasks that involve long conversations or lots of items can demand a fair amount of computing power. By handling these tasks directly on the device, you not only keep user data private but also reduce server load and cut down on serving costs.</p>
<h3 id="advantages-of-edge-ai">Advantages of Edge AI</h3>
<ul>
<li><strong>Latency</strong>: with Edge AI, there are no network round trips, so you save the time waiting for the server response over the network</li>
<li><strong>Offline-ready</strong>: Edge AI does not need network connectivity and thus can work in completely offline mode</li>
<li><strong>Privacy</strong>: all the data is processed on the device locally. Your personal data never leaves the device. Cloud AI exposes sensitive information to third parties</li>
<li><strong>Size</strong>: the models are reduced in size so that they can run on less powerful devices with a smaller memory footprint</li>
<li><strong>Power consumption</strong>: models are optimized to consume less compute and thus less power</li>
<li><strong>Cost</strong>:  All of the above advantages boil down to an important benefit - reduced cost. Since the inference happens locally, you don’t pay for input/output tokens. Compute, memory, and storage are moved to the local device instead of cloud infrastructure.</li>
</ul>
<h3 id="challenges-with-edge-ai">Challenges with Edge AI</h3>
<ul>
<li>Edge AI is limited by the device&rsquo;s capabilities such as memory, GPU, processor speed. Cloud AI is more powerful since it has bigger compute and larger storage.</li>
<li>Speed and compute are limited by the device capabilities and resources. Gemma 3 - xB model was 500MB. Gemma 3n is 2 GB and 4GB. Not practical to run on devices, unless there is a compelling use case. The latest Gemma 3n model can run on devices with 2 GB RAM. Example of such mobile phones -</li>
<li>Keeping the model up-to-date - integrate the learnings and more public data, download to the local device, update the version and make it available to the next inference instance</li>
</ul>
<h2 id="edge-ai---use-cases">Edge AI - Use cases</h2>
<ul>
<li>AI models embedded locally on wearable devices - can calculate metrics like blood pressure, heart rate, glucose levels using data points specific to the user combined with global data embedded in the model</li>
<li>Computer vision capabilities on security cameras and surveillance devices - real-time processing of object detection, suspicious activity detection</li>
<li>Smart devices like doorbells, thermostats etc. Facial recognition of the person at the door, adjust the whole house temperature. Can quickly process the data on-site and enhances privacy.</li>
</ul>
<h3 id="trade-off">Trade-off</h3>
<p><strong>Choosing between Edge AI and Cloud AI comes down to balancing privacy and real-time performance against device limitations.</strong> Edge AI keeps your data on-device, which is great for privacy and works offline, but is limited by the device’s memory, processing power, and storage. Cloud AI can handle more complex tasks and larger datasets, but requires sending data to external servers and depends on internet connectivity.</p>
<p>A classic example is facial recognition in photos: Edge AI ensures your photos never leave your device, while Cloud AI or self-hosted services like <a href="https://immich.app/" target="_blank" rel="noopener">Immich</a> can process and index large photo libraries faster and more efficiently, at the cost of some privacy.</p>
<h2 id="how-edge-ai-works">How Edge AI works</h2>
<p>The edge device downloads the model from the server or repository. Then when the application wants to use, it can load this model on-demand and run inference. This inference can happen completely offline.</p>
<p>You can take this one step further by managing your models centrally, downloading them to local devices on-demand, and running them locally.. Any issues or anomalies detected with the local model are sent to the server to diagnose and improve the quality of the model. The model is refined and the newer version is downloaded to the edge device and updated so that the next time you use it, you will use the newer model. This feedback loop gives the benefit of both - running models locally and keeping them up-to-date.</p>
<p>There are two main categories of tech available to build applications with on-device AI: the small models that can fit into the resource constraints of the devices, and the libraries and runtimes that run these models on constrained devices. Let&rsquo;s look into them next.</p>
<h2 id="small-language-models">Small Language Models</h2>
<p>At the heart of on-device AI are the models that can run on smaller devices - small enough to fit within the memory and processing limits of smartphones, wearables, and IoT gadgets - hence the name Small Language Models (SLMs). The most common SLMs for on-device AI are:</p>
<ul>
<li><strong>Gemini Nano</strong> - Google&rsquo;s flagship model that runs ML tasks on devices. This is proprietary model. <a href="https://developer.android.com/ai/gemini-nano" target="_blank" rel="noopener">Android Developers: Gemini Nano</a></li>
<li><strong>Gemma 3n</strong>: The latest iteration of the on-device version of Gemma family of models. Announced in Google I/O two weeks ago. This is open-weights model and in early preview mode. - <a href="https://ai.google.dev/gemma/docs/gemma-3n" target="_blank" rel="noopener">Google Gemma 3n documentation</a></li>
<li><strong>Gemma 3, 2</strong> - Google&rsquo;s stable on-device model with open weights. <a href="https://huggingface.co/google/gemma-3-12b-it" target="_blank" rel="noopener">Gemma 3 on Hugging Face</a></li>
<li><a href="https://azure.microsoft.com/en-us/products/phi" target="_blank" rel="noopener">Microsoft Phi</a>: A lightweight, open-source language model optimized for edge devices, developed by Microsoft.</li>
<li><a href="https://mistral.ai/news/" target="_blank" rel="noopener">Mistral</a>: A family of efficient, open-weight models designed for fast inference on resource-constrained hardware.</li>
</ul>
<h3 id="gemini-nano-vs-gemma-3n">Gemini Nano vs. Gemma 3n</h3>
<p>Prior to this Google I/O 2025, the flagship models for Edge AI are Gemma 3 and Nano. Both are similar models, optimized for on-device AI, but for different use cases.</p>
<table>
<thead>
<tr>
<th>Feature</th>
<th>Gemini Nano</th>
<th>Gemma 3n</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>Mobile Optimization</strong></td>
<td>Highly optimized for resource-constrained devices (e.g., &lt;4GB RAM devices)</td>
<td>Optimized for medium-tier devices (e.g., phones/tablets with 4-8GB RAM)</td>
</tr>
<tr>
<td><strong>Device Support</strong></td>
<td>Wearables, IoT, low-end Android phones</td>
<td>Android, iOS, tablets, laptops</td>
</tr>
<tr>
<td><strong>Model Access</strong></td>
<td>Proprietary<!-- raw HTML omitted -->Available via Android AICore/ML Kit GenAI APIs (restricted)</td>
<td>Open weights<!-- raw HTML omitted --><a href="https://www.kaggle.com/models/google/gemma" target="_blank" rel="noopener">Kaggle Model Hub</a> or <a href="https://huggingface.co/collections/google/gemma-3n-preview-682ca41097a31e5ac804d57b" target="_blank" rel="noopener">LiteRT HuggingFace community</a></td>
</tr>
<tr>
<td><strong>Integration Scope</strong></td>
<td>Google-first apps<!-- raw HTML omitted -->Select third-party apps with Google approval</td>
<td>Open integration for all third-party developers</td>
</tr>
<tr>
<td><strong>Use Cases</strong></td>
<td>- On-device AI features in Pixel devices<!-- raw HTML omitted -->- Google Messages Smart Reply</td>
<td>- Custom LLM apps<!-- raw HTML omitted -->- Cross-platform AI features<!-- raw HTML omitted -->- Local RAG implementations</td>
</tr>
<tr>
<td><strong>Feasibility</strong></td>
<td>Limited to Google-controlled use cases</td>
<td>Versatile for general app integration</td>
</tr>
</tbody>
</table>
<p>Despite these differences, both these models are built on the same foundational architecture, so we will likely see more and more convergence with different delivery and business models.</p>
<h2 id="libraries-and-runtime-to-run-on-device-ai-models">Libraries and Runtime to run on-device AI models</h2>
<p>The ecosystem for running on-device AI models consists of specialized libraries and runtimes. They make it possible to deploy and run machine learning models efficiently on smartphones, wearables, and other edge devices.</p>
<p>The main options available are:</p>
<ul>
<li><strong>Google AI Edge:</strong> Google’s collection of libraries, tools, and frameworks designed to deploy and run ML models directly on edge devices. It includes components like MediaPipe, LiteRT (formerly TensorFlow Lite), and ML Kit.</li>
<li><strong>TensorFlow Lite (now LiteRT):</strong> Google’s open-source runtime optimized for running machine learning models on mobile and embedded devices. LiteRT is the latest evolution, offering improved performance and flexibility.</li>
<li><strong>Apple Core ML:</strong> Apple’s framework for running ML models natively on iOS, macOS, tvOS, and watchOS devices, ensuring privacy and efficiency.</li>
</ul>
<p>Let’s take a closer look at each of these.</p>
<h3 id="google-ai-edge">Google AI Edge</h3>
<p>Google AI Edge is a comprehensive suite for deploying and running machine learning models on edge devices. It supports a wide range of platforms, including Android, iOS, browsers, and embedded systems. This encompasses various components.</p>
<p><strong>Gemini Nano</strong> is Google’s flagship model for on-device AI. While it is primarily targeted at Android devices, Google’s broader AI Edge SDK supports cross-platform deployment, including iOS, browsers, and embedded devices. You can learn more about it at  <a href="https://developer.android.com/ai/gemini-nano" target="_blank" rel="noopener">Google Gemini Nano for Android</a>.</p>
<p><strong>ML Kit</strong> provides prebuilt APIs and ready-to-use models for common features like barcode scanning, text recognition (OCR), face detection, image labeling, object detection and tracking, language identification, and smart reply. See more at <a href="https://developers.google.com/ml-kit" target="_blank" rel="noopener">ML Kit documentation</a></p>
<h4 id="tensorflow-lite-now-litert">TensorFlow Lite (now LiteRT)</h4>
<p>TensorFlow Lite is Google’s open-source runtime for running machine learning models on mobile and embedded devices. It has evolved into <strong>LiteRT</strong>, a toolkit optimized for running TensorFlow models efficiently on resource-constrained devices.</p>
<p>LiteRT allows you to run your own ML models - fine-tuned for specific tasks or data - efficiently on mobile, embedded, and edge devices. It supports hardware acceleration through specialized delegates or integration with platforms like Core ML.</p>
<p>Choose one of the following AI Edge frameworks:</p>
<ul>
<li><strong>LiteRT:</strong> Flexible and customizable runtime that can run a wide range of models. Choose a model for your use case, convert it to the LiteRT format (if necessary), and run it on-device. If you intend to use LiteRT, keep reading.</li>
<li><strong>MediaPipe Tasks:</strong> Plug-and-play solutions with default models that allow for customization. Choose the task that solves your AI/ML problem, and implement it on multiple platforms. If you intend to use MediaPipe Tasks, refer to the <a href="https://ai.google.dev/edge/mediapipe/solutions/tasks" target="_blank" rel="noopener">MediaPipe Tasks documentation</a>.</li>
</ul>
<p>You can learn more about this at <a href="https://developer.android.com/ai/overview" target="_blank" rel="noopener">AI guide for Android</a>.</p>
<h3 id="apple-core-ml">Apple Core ML</h3>
<p>Core ML is Apple’s framework for running machine learning models on iOS, macOS, tvOS, and watchOS devices. It is designed for on-device inference, minimizing memory footprint and power consumption while keeping data private. It leverages Apple silicon, including the CPU, GPU, and Neural Engine, to maximize speed and minimize memory footprint and power consumption.</p>
<p>It works with Vision (for image and video analysis), Natural Language (for text processing), and GameplayKit (for game AI). Models trained in TensorFlow, PyTorch, or other frameworks can be converted to Core ML format using Core ML Tools, making it straightforward to deploy existing models on Apple devices.</p>
<h4 id="how-coreml-works">How CoreML works</h4>
<ul>
<li><strong>Model Preparation:</strong> ML models are trained on powerful computers and converted to the Core ML format (<code>.mlmodel</code>), optimized for Apple hardware - using <a href="https://apple.github.io/coremltools/docs-guides/source/introductory-quickstart.html" target="_blank" rel="noopener">Core ML Tools</a></li>
<li><strong>Integration:</strong> The <code>.mlmodel</code> file is added to an Xcode project. Xcode automatically generates a Swift or Objective-C class for the model (<code>.mlmodelc</code>), providing type-safe access to inputs and outputs</li>
<li><strong>Execution:</strong> The app passes input data (e.g., images, text) to the model. Core ML dispatches computations to the most appropriate hardware (CPU, GPU, or Neural Engine) for efficient inference</li>
<li><strong>Stateful Models:</strong> Recent Core ML updates support stateful models (like those used in generative AI), managing state (such as key-value caches for language models) to improve efficiency and reduce overhead</li>
<li><strong>Performance Profiling:</strong> You can can profile model performance, identify bottlenecks, and optimize models for specific hardware using tools in Xcode and Core ML Tools</li>
</ul>
<p>Here is how you can use a model using Core ML in your app. Note - <code>.mlmodelc</code> is the compiled version of the model you imported into XCode.</p>
<div class="highlight"><div style="color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">
<table style="border-spacing:0;padding:0;margin:0;border:0;"><tr><td style="vertical-align:top;padding:0;margin:0;border:0;">
<pre tabindex="0" style="color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 1
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 2
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 3
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 4
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 5
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 6
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 7
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 8
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 9
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">10
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">11
</span></code></pre></td>
<td style="vertical-align:top;padding:0;margin:0;border:0;;width:100%">
<pre tabindex="0" style="color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-swift" data-lang="swift"><span style="display:flex;"><span>	<span style="color:#ff79c6">guard</span> <span style="color:#8be9fd;font-style:italic">let</span> <span style="color:#8be9fd;font-style:italic">modelURL</span> = Bundle.main.url(
</span></span><span style="display:flex;"><span>                            forResource: <span style="color:#f1fa8c">&#34;CoreMLTrainedModel&#34;</span>, 
</span></span><span style="display:flex;"><span>                            withExtension: <span style="color:#f1fa8c">&#34;mlmodelc&#34;</span>) <span style="color:#ff79c6">else</span> {
</span></span><span style="display:flex;"><span>	    fatalError(<span style="color:#f1fa8c">&#34;Failed to locate the Core ML model file.&#34;</span>)
</span></span><span style="display:flex;"><span>	}
</span></span><span style="display:flex;"><span>	<span style="color:#ff79c6">do</span> {
</span></span><span style="display:flex;"><span>	    <span style="color:#8be9fd;font-style:italic">let</span> <span style="color:#8be9fd;font-style:italic">model</span> = <span style="color:#ff79c6">try</span> MLModel(contentsOf: modelURL)
</span></span><span style="display:flex;"><span>	    <span style="color:#6272a4">// Now you have Core ML model object ready to make predictions</span>
</span></span><span style="display:flex;"><span>	} <span style="color:#ff79c6">catch</span> {
</span></span><span style="display:flex;"><span>	    fatalError(<span style="color:#f1fa8c">&#34;Failed to create Core ML model object: </span><span style="color:#f1fa8c">\(</span>error<span style="color:#f1fa8c">)</span><span style="color:#f1fa8c">&#34;</span>)
</span></span><span style="display:flex;"><span>	}
</span></span></code></pre></td></tr></table>
</div>
</div><p>After this, you prepare your input data so it matches the requirements of your model. This might involve resizing images, normalizing values, or encoding text. For example, you may need to resize an image to a specific dimension before passing it to the model. Then, pass the prepared input data to the model’s prediction method. The model will process the input and generate an output. Once you receive the model’s output, interpret and use it as needed in your app. Finally, display the results to the user.</p>
<p>Read more about it at <a href="https://apple.github.io/coremltools/docs-guides/source/introductory-quickstart.html" target="_blank" rel="noopener">CoreML Quick Start Guide</a>.</p>
<h2 id="gemma-3n">Gemma 3n</h2>
<p>This is the latest open model optimized to run on mobile devices with at least 2GB RAM. It provides real-time AI on phones, tablets and laptops. It is in early preview and is available on Google AI Studio, Google AI Edge SDK and <a href="https://huggingface.co/collections/google/gemma-3n-preview-682ca41097a31e5ac804d57b" target="_blank" rel="noopener">LiteRT HuggingFace community</a>.</p>
<p>Gemma 3n can be deployed on mobile devices using the MediaPipe library of Google AI Edge SDK that we saw earlier. It is built on the same advanced architecture that powers <strong>Gemini Nano</strong>, the flagship mobile-optimized model. The development of Gemma 3n is a collaboration between Google DeepMind, Qualcomm, and Samsung, ensuring that it is engineered for high performance and efficiency across a wide range of mobile hardware.</p>
<blockquote>
<p>Gemma 3n is the mobile-optimized, multimodal model that enables developers to build mobile-first AI applications - everything runs locally.</p>
</blockquote>
<h3 id="key-features">Key Features</h3>
<ul>
<li><strong>Natively Multimodal:</strong> Supports text, image, audio, and video inputs and outputs</li>
<li><strong>Multi-Lingual:</strong> Offers robust support for multiple languages - trained over 140 languages</li>
<li><strong>Open Weights:</strong> The model is available under open weights, encouraging community development and transparency</li>
<li><strong>Realtime Capabilities:</strong> Enables real-time transcription, translation, and voice interactions directly on device</li>
<li><strong>Extended Context Length:</strong> Supports a context window of up to 32K tokens, significant input length for complex and lengthy data</li>
</ul>
<h3 id="model-variants">Model variants</h3>
<p>Two main variants are available in the preview:</p>
<ol>
<li><strong>Gemma 3n E2B:</strong> <a href="https://huggingface.co/google/gemma-3n-E2B-it-litert-preview" target="_blank" rel="noopener">HuggingFace Model Hub Link</a></li>
<li><strong>Gemma 3n E4B:</strong> <a href="https://huggingface.co/google/gemma-3n-E4B-it-litert-preview" target="_blank" rel="noopener">HuggingFace Model Hub Link</a></li>
</ol>
<p>For reference, the previous model <strong>Gemma 3-1B</strong> is available at <a href="https://huggingface.co/litert-community/Gemma3-1B-IT" target="_blank" rel="noopener">https://huggingface.co/litert-community/Gemma3-1B-IT</a></p>
<h3 id="naming-is-hard-but-fun-too">Naming is hard, but fun too!</h3>
<p>There are a few subtle elements in the name of the Gemma models that are interesting to know.</p>
<ul>
<li>The <strong>n</strong> suffix in the model name refers to the <em>nano-sized</em> models in the Gemma family, specifically optimized for resource-constrained devices like mobile phones and wearables</li>
<li>The <strong>E</strong> prefix in the Gemma 3n model names stands for the <em>Effective</em> parameter size, indicating the reduced, or effective, number of parameters the model actually needs to use during inference on a device, rather than the total parameter count stored in the model files. For example, the Gemma 3n E2B model contains 5 billion parameters, but it can operate with an effective memory footprint of 2B parameters because of the techniques like Per-Layer Embedding (PLE) caching and selective activation of parameters. Similarly the E4B model needs only 4B parameters to be active at run time even though it is a 8B parameter model.</li>
</ul>
<p>LiteRT is the new name for <em>TensorFlowLite</em> - and it stands for <em>Lite RunTime</em></p>
<h3 id="architecture-of-gemma-3n">Architecture of Gemma 3n</h3>
<p>There are a few important aspects of the new architecture that powers Gemma 3n and the Gemini Nano models.</p>
<ul>
<li><strong>PLE Caching:</strong> PLE (Per-Layer Embedding) parameters are generated separately and cached in fast storage, then added during inference, reducing the memory footprint at runtime</li>
<li><strong>Selective Parameter Activation:</strong> The model can dynamically load only the parameters needed for the current task (e.g., text, image, or audio), further reducing resource use. The model with 4B active memory footprint embeds a nested state-of-the-art submodel with 2B active memory footprint. This allows the model to dynamically load the relevant submodel to balance performance and quality based on device capabilities, user query etc.</li>
<li><strong>MatFormer Architecture:</strong> This allows the model to nest smaller submodels within a larger one, so only the relevant submodel is activated per request, minimizing compute and memory requirements</li>
</ul>
<h2 id="hands-on-with-gemma-3n">Hands-on with Gemma 3n</h2>
<p>Ok, let’s roll up our sleeves and see these models in action. We can do that in a few places:</p>
<ol>
<li><strong>Google AI Studio:</strong> This is Google’s playground where you can try all their models in a chat-like interface. It’s the simplest way to see Gemma models, including the latest Gemma 3n.</li>
<li><strong>Sample apps from MediaPipe:</strong> These are available for Android, iOS, and web browsers. They let you run models directly on your device or in the browser.</li>
<li><strong>Google AI Edge Gallery app:</strong> This new app from Google AI Edge SDK lets you download and run models like Gemma 3n directly on your Android device, offline.</li>
</ol>
<p><strong>Note:</strong> The mobile setup is fairly easy. You don’t need advanced mobile expertise - just a compatible USB cable and the ability to follow instructions.</p>
<h3 id="1-run-on-google-ai-studio">1. Run on Google AI Studio</h3>
<p>This is the fastest way to try out Gemma 3n without any setup hassle.</p>
<ul>
<li>Go to <a href="https://aistudio.google.com/prompts/new_chat" target="_blank" rel="noopener">Google AI Studio</a> and login with your Google account</li>
<li>Select the <strong>Gemma 3n 4B</strong> model in the dropdown menu on the right.</li>
<li>Start chatting with the model right away. Adjust the parameters like temperature, TopP etc. as you wish.</li>
<li>Other Gemma models are also available here. You can use any of them.</li>
</ul>
<p>Here is a screenshot of Google AI Studio running Gemma 3n 4B. As you can see, the answer to my question about a new book on heart health didn&rsquo;t come out correctly. This is a limitation of smaller models.









    
        
        
        
        
        
        
        <figure style="margin-inline-start: 10px; margin-inline-end: 10px; margin-left: auto; margin-right: auto; max-width: 100%; text-align: center;">
            <a href="https://annjose.com/post/mobile-ondevice-ai/gemma-3n-ai-studio.png" target="_blank">
                <img srcset="https://annjose.com/post/mobile-ondevice-ai/gemma-3n-ai-studio_hueedcda6fc31da7c3452e64fcc76304a7_1699561_320x0_resize_box_3.png 320w, https://annjose.com/post/mobile-ondevice-ai/gemma-3n-ai-studio_hueedcda6fc31da7c3452e64fcc76304a7_1699561_480x0_resize_box_3.png 480w, https://annjose.com/post/mobile-ondevice-ai/gemma-3n-ai-studio_hueedcda6fc31da7c3452e64fcc76304a7_1699561_768x0_resize_box_3.png 768w, https://annjose.com/post/mobile-ondevice-ai/gemma-3n-ai-studio_hueedcda6fc31da7c3452e64fcc76304a7_1699561_1024x0_resize_box_3.png 1024w, https://annjose.com/post/mobile-ondevice-ai/gemma-3n-ai-studio_hueedcda6fc31da7c3452e64fcc76304a7_1699561_1500x0_resize_box_3.png 1500w"
                     sizes="(max-width: 320px) 90vw, (max-width: 480px) 85vw, (max-width: 768px) 80vw, (max-width: 1200px) 60vw, 50vw"
                     src="https://annjose.com/post/mobile-ondevice-ai/gemma-3n-ai-studio.png"
                     alt=""  
                     style="max-width: 100%; height: auto;" loading="lazy">
            </a>
            
        </figure>

    
</p>
<h3 id="2-run-on-android-phones">2. Run on Android phones</h3>
<p>We have two options to run Gemma models on Android:</p>
<h4 id="21-mediapipe-sample-app">2.1 MediaPipe Sample App</h4>
<p>This app demonstrates all supported Small Language Models (SLMs) like Gemma 2, Gemma 3, Microsoft Phi, and QWEN. It’s available on <a href="https://github.com/google-ai-edge/mediapipe-samples" target="_blank" rel="noopener">GitHub: google-ai-edge/mediapipe-samples</a>.
- <strong>Drawbacks:</strong> The sample app is minimal, somewhat outdated, and not as polished as newer offerings. Importantly, <strong>you cannot run Gemma 3n models in this app</strong>. Attempts to modify the source code to add Gemma 3n often result in errors like “Access denied. Please try again and grant the necessary permissions,” even after setting up permissions on Hugging Face. In my understanding, this suggests the app is explicitly restricted from running newer models.</p>
<p>Here are some screenshots of MediaPipe sample app running on my Pixel 8 Pro Android device:


<div class="pure-g">

  
  
  
  
  <div class="pure-u-1 pure-u-md-1-2">
    <div style="padding: 0 .2em">
      <img
        class="pure-img-responsive"
        src="media-pipe-android-app-models.png"
        alt="MediaPipe sample with Gemma 3 - 1B">
    </div>
  </div>
  

  
  
  
  
  <div class="pure-u-1 pure-u-md-1-2">
    <div style="padding: 0 .2em">
      <img
        class="pure-img-responsive"
        src="mediapipe-android-chat-gemma-3-1B.png"
        alt="MediaPipe sample with Gemma 3 - 1B">
    </div>
  </div>
  

</div>
</p>
<p>And the sample app running in Adroid studio









    
        
        
        
        
        
        
        <figure style="margin-inline-start: 10px; margin-inline-end: 10px; margin-left: auto; margin-right: auto; max-width: 100%; text-align: center;">
            <a href="https://annjose.com/post/mobile-ondevice-ai/mediapipe-android-app-AndroidStudio.png" target="_blank">
                <img srcset="https://annjose.com/post/mobile-ondevice-ai/mediapipe-android-app-AndroidStudio_hucb095717916dc5310ad011c7e75222bd_1263277_320x0_resize_box_3.png 320w, https://annjose.com/post/mobile-ondevice-ai/mediapipe-android-app-AndroidStudio_hucb095717916dc5310ad011c7e75222bd_1263277_480x0_resize_box_3.png 480w, https://annjose.com/post/mobile-ondevice-ai/mediapipe-android-app-AndroidStudio_hucb095717916dc5310ad011c7e75222bd_1263277_768x0_resize_box_3.png 768w, https://annjose.com/post/mobile-ondevice-ai/mediapipe-android-app-AndroidStudio_hucb095717916dc5310ad011c7e75222bd_1263277_1024x0_resize_box_3.png 1024w, https://annjose.com/post/mobile-ondevice-ai/mediapipe-android-app-AndroidStudio_hucb095717916dc5310ad011c7e75222bd_1263277_1500x0_resize_box_3.png 1500w"
                     sizes="(max-width: 320px) 90vw, (max-width: 480px) 85vw, (max-width: 768px) 80vw, (max-width: 1200px) 60vw, 50vw"
                     src="https://annjose.com/post/mobile-ondevice-ai/mediapipe-android-app-AndroidStudio.png"
                     alt=""  
                     style="max-width: 100%; height: auto;" loading="lazy">
            </a>
            
        </figure>

    
</p>
<h4 id="22-google-ai-edge-gallery-app">2.2 Google AI Edge Gallery App</h4>
<p>This is a newer, more versatile sample app available as an experimental Alpha release. It’s sleek, easy to use, and demonstrates the full power of on-device AI. You can run Gemma 3n models (E2B and E4B) directly on your device, but not older models. <br>
- <strong>Download and instructions:</strong> <a href="https://github.com/google-ai-edge/gallery" target="_blank" rel="noopener">GitHub: google-ai-edge/gallery</a>
- <strong>How it works:</strong> After installing, select a model from the list (such as Gemma 3n E2B or E4B), and you can start using AI features like chat, image analysis, and more - all offline</p>
<p>Here are some screenshots of Gallery app running on my Pixel 8 Pro Android device - the home page that shows the tasks it supports, asking the user to review the license of Gemma 3n and downloading the Gemma 3n model:</p>


<div class="pure-g">

  
  
  
  
  <div class="pure-u-1 pure-u-md-1-3">
    <div style="padding: 0 .2em">
      <img
        class="pure-img-responsive"
        src="gallery-app-home.png"
        alt="Edge Gallery App Home">
    </div>
  </div>
  

  
  
  
  
  <div class="pure-u-1 pure-u-md-1-3">
    <div style="padding: 0 .2em">
      <img
        class="pure-img-responsive"
        src="gallery-app-hf-license.png"
        alt="Asking review license">
    </div>
  </div>
  

  
  
  
  
  <div class="pure-u-1 pure-u-md-1-3">
    <div style="padding: 0 .2em">
      <img
        class="pure-img-responsive"
        src="gallery-app-downloading.png"
        alt="Downloading the model">
    </div>
  </div>
  

</div>

<p>The gallery app supports three three main type of tasks - Chat, Explain Image and Summarize.


<div class="pure-g">

  
  
  
  
  <div class="pure-u-1 pure-u-md-1-3">
    <div style="padding: 0 .2em">
      <img
        class="pure-img-responsive"
        src="gemma-3n-chat-google-edge-gallery.png"
        alt="Chat using Gemma 3n">
    </div>
  </div>
  

  
  
  
  
  <div class="pure-u-1 pure-u-md-1-3">
    <div style="padding: 0 .2em">
      <img
        class="pure-img-responsive"
        src="gemma-3n-explain-image-google-edge-gallery.png"
        alt="Explain Image using Gemma 3n">
    </div>
  </div>
  

  
  
  
  
  <div class="pure-u-1 pure-u-md-1-3">
    <div style="padding: 0 .2em">
      <img
        class="pure-img-responsive"
        src="gemma-3n-summarize-google-edge-gallery.png"
        alt="Summarize using Gemma 3n">
    </div>
  </div>
  

</div>
</p>
<p>See detailed instructions for each app:</p>
<ul>
<li><strong>MediaPipe Sample App:</strong> <a href="https://github.com/google-ai-edge/mediapipe-samples" target="_blank" rel="noopener">MediaPipe Samples GitHub</a></li>
<li><strong>Gallery App:</strong> <a href="https://github.com/google-ai-edge/gallery" target="_blank" rel="noopener">Google AI Edge Gallery GitHub</a>
In summary, use the old MediaPipe sample app to run models prior to Gemma 3n, and the new Gallery app to run Gemma 3n models. Google may consolidate these apps in the future or deprecate older models.</li>
</ul>
<h3 id="3-run-on-ios-phones">3. Run on iOS phones</h3>
<p><strong>The new Google AI Edge Gallery app is not yet available for iOS - its release is marked as “coming soon.”</strong><br>
Currently, our only option is to use the old MediaPipe sample app, which supports models prior to Gemma 3n (like Gemma 2 and Gemma 3).</p>
<p>The LLM Inference API from MediaPipe enables you to run large language models (LLMs) entirely on-device for iOS applications. This allows you to perform advanced AI tasks such as text generation, information retrieval, and document summarization - all while keeping user data private and secure.</p>
<p><strong>How to get started:</strong></p>
<ol>
<li><strong>Install the MediaPipeTasksGenai library</strong>  - Add the necessary dependencies to your project using CocoaPods. Add the following to your <code>Podfile</code>:</li>
</ol>
<div class="highlight"><div style="color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">
<table style="border-spacing:0;padding:0;margin:0;border:0;"><tr><td style="vertical-align:top;padding:0;margin:0;border:0;">
<pre tabindex="0" style="color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">1
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">2
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">3
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">4
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">5
</span></code></pre></td>
<td style="vertical-align:top;padding:0;margin:0;border:0;;width:100%">
<pre tabindex="0" style="color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-ruby" data-lang="ruby"><span style="display:flex;"><span>target <span style="color:#f1fa8c">&#39;MyLlmInferenceApp&#39;</span> <span style="color:#ff79c6">do</span>
</span></span><span style="display:flex;"><span>  use_frameworks!
</span></span><span style="display:flex;"><span>  pod <span style="color:#f1fa8c">&#39;MediaPipeTasksGenAI&#39;</span>
</span></span><span style="display:flex;"><span>  pod <span style="color:#f1fa8c">&#39;MediaPipeTasksGenAIC&#39;</span>
</span></span><span style="display:flex;"><span><span style="color:#ff79c6">end</span>
</span></span></code></pre></td></tr></table>
</div>
</div><p>Note - The library is compatible with both Swift and Objective-C apps and does not require any additional language-specific setup.</p>
<ol start="2">
<li>
<p><strong>Download a compatible model</strong>   - Download a supported model (such as Gemma-2 2B in 8-bit quantized format) from Kaggle Models. Add the model file to your Xcode project.</p>
</li>
<li>
<p><strong>Initialize the LLM Inference task</strong>  - Configure the task with your model path and desired options:</p>
</li>
</ol>
<div class="highlight"><div style="color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">
<table style="border-spacing:0;padding:0;margin:0;border:0;"><tr><td style="vertical-align:top;padding:0;margin:0;border:0;">
<pre tabindex="0" style="color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 1
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 2
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 3
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 4
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 5
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 6
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 7
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 8
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 9
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">10
</span></code></pre></td>
<td style="vertical-align:top;padding:0;margin:0;border:0;;width:100%">
<pre tabindex="0" style="color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-swift" data-lang="swift"><span style="display:flex;"><span><span style="color:#8be9fd;font-style:italic">import</span> <span style="color:#50fa7b">MediaPipeTasksGenai</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#8be9fd;font-style:italic">let</span> <span style="color:#8be9fd;font-style:italic">modelPath</span> = Bundle.main.path(forResource: <span style="color:#f1fa8c">&#34;model&#34;</span>, ofType: <span style="color:#f1fa8c">&#34;bin&#34;</span>)
</span></span><span style="display:flex;"><span><span style="color:#8be9fd;font-style:italic">let</span> <span style="color:#8be9fd;font-style:italic">options</span> = LlmInferenceOptions()
</span></span><span style="display:flex;"><span>options.baseOptions.modelPath = modelPath
</span></span><span style="display:flex;"><span>options.maxTokens = <span style="color:#bd93f9">1000</span>
</span></span><span style="display:flex;"><span>options.topk = <span style="color:#bd93f9">40</span>
</span></span><span style="display:flex;"><span>options.temperature = <span style="color:#bd93f9">0.8</span>
</span></span><span style="display:flex;"><span>options.randomSeed = <span style="color:#bd93f9">101</span>
</span></span><span style="display:flex;"><span><span style="color:#8be9fd;font-style:italic">let</span> <span style="color:#8be9fd;font-style:italic">llmInference</span> = <span style="color:#ff79c6">try</span> LlmInference(options: options)
</span></span></code></pre></td></tr></table>
</div>
</div><ol start="4">
<li><strong>Run the task and generate responses</strong>  - Use the <code>generateResponse(inputText:)</code> method for single responses, or <code>generateResponseAsync(inputText:)</code> for streaming:</li>
</ol>
<div class="highlight"><div style="color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">
<table style="border-spacing:0;padding:0;margin:0;border:0;"><tr><td style="vertical-align:top;padding:0;margin:0;border:0;">
<pre tabindex="0" style="color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 1
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 2
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 3
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 4
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 5
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 6
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 7
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 8
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 9
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">10
</span></code></pre></td>
<td style="vertical-align:top;padding:0;margin:0;border:0;;width:100%">
<pre tabindex="0" style="color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-swift" data-lang="swift"><span style="display:flex;"><span><span style="color:#8be9fd;font-style:italic">let</span> <span style="color:#8be9fd;font-style:italic">result</span> = <span style="color:#ff79c6">try</span> LlmInference.generateResponse(inputText: inputPrompt)
</span></span><span style="display:flex;"><span><span style="color:#6272a4">// For streaming:</span>
</span></span><span style="display:flex;"><span><span style="color:#8be9fd;font-style:italic">let</span> <span style="color:#8be9fd;font-style:italic">resultStream</span> = LlmInference.generateResponseAsync(inputText: inputPrompt)
</span></span><span style="display:flex;"><span><span style="color:#ff79c6">do</span> {
</span></span><span style="display:flex;"><span>  <span style="color:#ff79c6">for</span> <span style="color:#ff79c6">try</span> await partialResult <span style="color:#ff79c6">in</span> resultStream {
</span></span><span style="display:flex;"><span>    print(<span style="color:#f1fa8c">&#34;</span><span style="color:#f1fa8c">\(</span>partialResult<span style="color:#f1fa8c">)</span><span style="color:#f1fa8c">&#34;</span>)
</span></span><span style="display:flex;"><span>  }
</span></span><span style="display:flex;"><span>} <span style="color:#ff79c6">catch</span> {
</span></span><span style="display:flex;"><span>  print(<span style="color:#f1fa8c">&#34;Response error: </span><span style="color:#f1fa8c">\(</span>error<span style="color:#f1fa8c">)</span><span style="color:#f1fa8c">&#34;</span>)
</span></span><span style="display:flex;"><span>}
</span></span></code></pre></td></tr></table>
</div>
</div><ol start="5">
<li><strong>More configuration</strong> - You can adjust the model parameters like <strong>maxTokens</strong>, <strong>topk</strong>, <strong>temperature</strong>, <strong>randomSeed</strong> and path to the model file as <strong>modelPath</strong></li>
<li><strong>Sample application</strong> - The <a href="https://github.com/google-ai-edge/mediapipe-samples" target="_blank" rel="noopener">MediaPipe Samples GitHub repository</a> includes a sample iOS app that demonstrates how to use the LLM Inference API. Clone the repository and open the project in Xcode to see it in action.</li>
</ol>
<p>The API also supports LoRA (Low-Rank Adaptation) for efficient fine-tuning of models, allowing you to customize model behavior for specific tasks.</p>
<p>For detailed instructions and advanced setup, see the official documentation: <a href="https://ai.google.dev/edge/mediapipe/solutions/genai/llm_inference/ios" target="_blank" rel="noopener">MediaPipe LLM Inference API for iOS</a></p>
<h3 id="4-run-on-mobile-browsers">4. Run on mobile browsers</h3>
<p>You can run Gemma and other large language models directly in your browser using the MediaPipe LLM Inference API for JavaScript. This allows you to perform advanced AI tasks - such as text generation, information retrieval, and document summarization - entirely on-device, without sending data to a server.</p>
<p><strong>Supported Models</strong></p>
<ul>
<li><strong>Gemma-2 2B</strong>, <strong>Gemma-3 1B</strong>, and other models like Phi-2 and StableLM are supported.</li>
<li><strong>Gemma 3n (E2B, E4B)</strong> is not currently listed as supported for the Web API.</li>
</ul>
<p>It is fascinating to see the model loaded into the browser on your machine. Of course, tools like Ollama does the same, but it feels different when you see that happen in the context of a browser - which you usually think of as a lightweight window to the world wide web.</p>
<p><strong>Setup instructions</strong></p>
<ul>
<li><strong>Install the package:</strong></li>
</ul>
<div class="highlight"><div style="color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">
<table style="border-spacing:0;padding:0;margin:0;border:0;"><tr><td style="vertical-align:top;padding:0;margin:0;border:0;">
<pre tabindex="0" style="color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">1
</span></code></pre></td>
<td style="vertical-align:top;padding:0;margin:0;border:0;;width:100%">
<pre tabindex="0" style="color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-bash" data-lang="bash"><span style="display:flex;"><span>    npm install @mediapipe/tasks-genai
</span></span></code></pre></td></tr></table>
</div>
</div><p>Or use the CDN:</p>
<div class="highlight"><div style="color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">
<table style="border-spacing:0;padding:0;margin:0;border:0;"><tr><td style="vertical-align:top;padding:0;margin:0;border:0;">
<pre tabindex="0" style="color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">1
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">2
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">3
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">4
</span></code></pre></td>
<td style="vertical-align:top;padding:0;margin:0;border:0;;width:100%">
<pre tabindex="0" style="color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-xml" data-lang="xml"><span style="display:flex;"><span>	<span style="color:#ff79c6">&lt;script</span> 
</span></span><span style="display:flex;"><span>        <span style="color:#50fa7b">src=</span><span style="color:#f1fa8c">&#34;https://cdn.jsdelivr.net/npm/@mediapipe/tasks-genai/genai_bundle.cjs&#34;</span> 
</span></span><span style="display:flex;"><span>        <span style="color:#50fa7b">crossorigin=</span><span style="color:#f1fa8c">&#34;anonymous&#34;</span><span style="color:#ff79c6">&gt;</span>
</span></span><span style="display:flex;"><span>    <span style="color:#ff79c6">&lt;/script&gt;</span>
</span></span></code></pre></td></tr></table>
</div>
</div><ul>
<li>
<p><strong>Download a model:</strong>  For example, download Gemma-2 2B in 8-bit quantized format from Kaggle Models or HuggingFace LiteRT and place it in your project directory.</p>
</li>
<li>
<p><strong>Initialize the task:</strong></p>
</li>
</ul>
<div class="highlight"><div style="color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">
<table style="border-spacing:0;padding:0;margin:0;border:0;"><tr><td style="vertical-align:top;padding:0;margin:0;border:0;">
<pre tabindex="0" style="color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 1
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 2
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 3
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 4
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 5
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 6
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 7
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 8
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 9
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">10
</span></code></pre></td>
<td style="vertical-align:top;padding:0;margin:0;border:0;;width:100%">
<pre tabindex="0" style="color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-javascript" data-lang="javascript"><span style="display:flex;"><span>    <span style="color:#ff79c6">const</span> genai <span style="color:#ff79c6">=</span> <span style="color:#ff79c6">await</span> FilesetResolver.forGenAiTasks(
</span></span><span style="display:flex;"><span>	  <span style="color:#f1fa8c">&#34;https://cdn.jsdelivr.net/npm/@mediapipe/tasks-genai@latest/wasm&#34;</span>
</span></span><span style="display:flex;"><span>	);
</span></span><span style="display:flex;"><span>	llmInference <span style="color:#ff79c6">=</span> <span style="color:#ff79c6">await</span> LlmInference.createFromOptions(genai, {
</span></span><span style="display:flex;"><span>	  baseOptions<span style="color:#ff79c6">:</span> { modelAssetPath<span style="color:#ff79c6">:</span> <span style="color:#f1fa8c">&#39;/assets/gemma-2b-it-gpu-int8.bin&#39;</span> },
</span></span><span style="display:flex;"><span>	  maxTokens<span style="color:#ff79c6">:</span> <span style="color:#bd93f9">1000</span>,
</span></span><span style="display:flex;"><span>	  topK<span style="color:#ff79c6">:</span> <span style="color:#bd93f9">40</span>,
</span></span><span style="display:flex;"><span>	  temperature<span style="color:#ff79c6">:</span> <span style="color:#bd93f9">0.8</span>,
</span></span><span style="display:flex;"><span>	  randomSeed<span style="color:#ff79c6">:</span> <span style="color:#bd93f9">101</span>
</span></span><span style="display:flex;"><span>	});
</span></span></code></pre></td></tr></table>
</div>
</div><ul>
<li><strong>Run inference:</strong></li>
</ul>
<div class="highlight"><div style="color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">
<table style="border-spacing:0;padding:0;margin:0;border:0;"><tr><td style="vertical-align:top;padding:0;margin:0;border:0;">
<pre tabindex="0" style="color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">1
</span></code></pre></td>
<td style="vertical-align:top;padding:0;margin:0;border:0;;width:100%">
<pre tabindex="0" style="color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-javascript" data-lang="javascript"><span style="display:flex;"><span>	<span style="color:#ff79c6">const</span> response <span style="color:#ff79c6">=</span> <span style="color:#ff79c6">await</span> llmInference.generateResponse(inputPrompt);
</span></span></code></pre></td></tr></table>
</div>
</div><p>Or stream the output:</p>
<div class="highlight"><div style="color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">
<table style="border-spacing:0;padding:0;margin:0;border:0;"><tr><td style="vertical-align:top;padding:0;margin:0;border:0;">
<pre tabindex="0" style="color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">1
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">2
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">3
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">4
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">5
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">6
</span></code></pre></td>
<td style="vertical-align:top;padding:0;margin:0;border:0;;width:100%">
<pre tabindex="0" style="color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-javascript" data-lang="javascript"><span style="display:flex;"><span>	llmInference.generateResponse(
</span></span><span style="display:flex;"><span>        inputPrompt,
</span></span><span style="display:flex;"><span>        (partialResult, done) =&gt; {
</span></span><span style="display:flex;"><span>            <span style="color:#8be9fd;font-style:italic">document</span>.getElementById(<span style="color:#f1fa8c">&#39;output&#39;</span>).textContent <span style="color:#ff79c6">+=</span> partialResult;
</span></span><span style="display:flex;"><span>        }
</span></span><span style="display:flex;"><span>    );
</span></span></code></pre></td></tr></table>
</div>
</div><ul>
<li><strong>Browser Compatibility</strong> - Requires WebGPU support. Check the <a href="https://ai.google.dev/edge/mediapipe/solutions/genai/llm_inference/web_js#gpu_browser_compatibility" target="_blank" rel="noopener">GPU browser compatibility</a> page for details.</li>
<li><strong>LoRA (Low-Rank Adaptation)</strong> - You can fine-tune models using LoRA and load custom LoRA weights at runtime for specialized tasks.</li>
</ul>
<p>See detailed instructions: <a href="https://ai.google.dev/edge/mediapipe/solutions/genai/llm_inference/web_js" target="_blank" rel="noopener">MediaPipe LLM Inference API for Web</a></p>
<h3 id="cross-platform-summary">Cross platform summary</h3>
<p>The following table summarizes how to run the model on various platforms.</p>
<table>
<thead>
<tr>
<th>Platform</th>
<th>Supported Gemma Models</th>
<th>Recommended App/Interface</th>
<th>Notes</th>
</tr>
</thead>
<tbody>
<tr>
<td>Google AI Studio</td>
<td>Gemma 3n, others</td>
<td>Web browser</td>
<td>Easiest, no setup required</td>
</tr>
<tr>
<td>Android</td>
<td>Prior to Gemma 3n</td>
<td>MediaPipe Sample App (GitHub)</td>
<td>Outdated, not Gemma 3n</td>
</tr>
<tr>
<td>Android</td>
<td>Gemma 3n</td>
<td>Google AI Edge Gallery App</td>
<td>New, sleek, supports Gemma 3n</td>
</tr>
<tr>
<td>iOS</td>
<td>Prior to Gemma 3n</td>
<td>MediaPipe Sample App (GitHub)</td>
<td>No Gemma 3n, Gallery app coming soon</td>
</tr>
<tr>
<td>Web browser</td>
<td>Prior to Gemma 3n</td>
<td>MediaPipe LLM Inference API (JS)</td>
<td>Runs in browser, not Gemma 3n</td>
</tr>
</tbody>
</table>
<h2 id="references">References</h2>
<ul>
<li><a href="https://developers.googleblog.com/en/introducing-gemma-3n/" target="_blank" rel="noopener">Announcement of Gemma 3n at Google I/O 2025</a></li>
<li>Google Gemini Nano - a good starting point for on-device ML for Android - <a href="https://developer.android.com/ai/gemini-nano" target="_blank" rel="noopener">https://developer.android.com/ai/gemini-nano</a></li>
<li>AI guide for Android - <a href="https://developer.android.com/ai/overview" target="_blank" rel="noopener">https://developer.android.com/ai/overview</a></li>
<li>Quick start guide of Core ML - <a href="https://apple.github.io/coremltools/docs-guides/source/introductory-quickstart.html" target="_blank" rel="noopener">https://apple.github.io/coremltools/docs-guides/source/introductory-quickstart.html</a></li>
<li><a href="https://www.ibm.com/think/topics/edge-ai" target="_blank" rel="noopener">Simple intro to Edge AI - by IBM</a></li>
</ul>


  
<div class="prev-next-post pure-g">
  <div class="pure-u-1-24" style="text-align: left;">
    
    <a href="https://annjose.com/post/cloudflare-autorag-step-by-step/"><i class="fa fa-chevron-left"></i></a>
    
  </div>
  <div class="pure-u-10-24">
    
    <nav class="prev">
      <a href="https://annjose.com/post/cloudflare-autorag-step-by-step/">Cloudflare AutoRAG: RAG on auto-pilot</a>
    </nav>
    
  </div>
  <div class="pure-u-2-24">
    &nbsp;
  </div>
  <div class="pure-u-10-24">
    
  </div>
  <div class="pure-u-1-24" style="text-align: right;">
    
  </div>
</div>



  
<div id="disqus_thread"></div>
<script type="text/javascript">

(function() {
    
    
    if (window.location.hostname == "localhost")
        return;

    var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
    var disqus_shortname = 'anncjose';
    dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
    (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
})();
</script>
<noscript>Please enable JavaScript to view the <a href="http://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
<a href="http://disqus.com/" class="dsq-brlink">comments powered by <span class="logo-disqus">Disqus</span></a>


</div>

</div>
</div>
<script src="https://annjose.com/js/ui.js"></script>
<script type="text/javascript"
  src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>

<script type="text/x-mathjax-config">
MathJax.Hub.Config({
  tex2jax: {
    inlineMath: [['$','$'], ['\\(','\\)']],
    displayMath: [['$$','$$'], ['\[','\]']],
    processEscapes: true,
    processEnvironments: true,
    skipTags: ['script', 'noscript', 'style', 'textarea', 'pre'],
    TeX: { equationNumbers: { autoNumber: "AMS" },
         extensions: ["AMSmath.js", "AMSsymbols.js"] }
  }
});
</script>




<script async src="https://www.googletagmanager.com/gtag/js?id=G-2PEL4BVYJE"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag() { dataLayer.push(arguments); }
  gtag('js', new Date());

  gtag('config', 'G-2PEL4BVYJE');
</script>




<script 
    id="counterscale-script" 
    data-site-id="annjose-blog"
    src="https://counterscale.annjose.workers.dev/tracker.js" 
    defer>
</script>

</body>
</html>

